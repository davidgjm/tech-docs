= Performance monitoring
:toc:
:source-highlighter: rouge
:icons: font

.icon:sticky-note[2x, role=lime] Open Questions
====
* How to achieve this with Spring Cloud/Spring Boot on Kubernetes in Azure?
====

System performance depends on a number of factors. Each factor is typically measured through key performance indicators (KPIs), such as the number of database transactions per second or the volume of network requests that are successfully serviced in a specified time frame. Some of these KPIs might be available as specific performance measures, whereas others might be derived from a combination of metrics.

[NOTE]
====
Determining poor or good performance requires that you understand the level of performance at which the system should be capable of running. This requires observing the system while it's functioning under a typical load and capturing the data for each KPI over a period of time. This might involve running the system under a simulated load in a test environment and gathering the appropriate data before deploying the system to a production environment.

You should also ensure that monitoring for performance purposes does not become a burden on the system. You might be able to dynamically adjust the level of detail for the data that the performance monitoring process gathers.
====

== Requirements for performance monitoring
To examine system performance, an operator typically needs to see information that includes:

* The response rates for user requests.
* The number of concurrent user requests.
* The volume of network traffic.
* The rates at which business transactions are being completed.
* The average processing time for requests.

It can also be helpful to provide tools that enable an operator to help spot correlations, such as:

* The number of concurrent users versus request latency times (how long it takes to start processing a request after the user has sent it).
* The number of concurrent users versus the average response time (how long it takes to complete a request after it has started processing).
* The volume of requests versus the number of processing errors.

Along with this high-level functional information, an operator should be able to obtain a detailed view of the performance for *each component* in the system. This data is typically provided through low-level performance counters that track information such as:

* Memory utilization.
* Number of threads.
* CPU processing time.
* Request queue length.
* Disk or network I/O rates and errors.
* Number of bytes written or read.
* Middleware indicators, such as queue length.

All visualizations should allow an operator to specify a time period. The displayed data might be a snapshot of the current situation and/or a historical view of the performance.

An operator should be able to raise an alert based on any performance measure for any specified value during any specified time interval.

== Data sources, instrumentation, and data-collection requirements

You can gather high-level performance data (throughput, number of concurrent users, number of business transactions, error rates, and so on) by monitoring the progress of users' requests as they arrive and pass through the system. This involves incorporating tracing statements at key points in the application code, together with timing information. All faults, exceptions, and warnings should be captured with sufficient data for correlating them with the requests that caused them.

If possible, you should also capture performance data for any external systems that the application uses. These external systems might provide their own performance counters or other features for requesting performance data. If this is not possible, record information such as the start time and end time of each request made to an external system, together with the status (success, fail, or warning) of the operation. For example, you can use a stopwatch approach to time requests: start a timer when the request starts and then stop the timer when the request finishes.

Low-level performance data for individual components in a system might be available through features and services such as Windows performance counters and Azure Diagnostics.

== Analyzing performance data

Much of the analysis work consists of aggregating performance data by user request type and/or the subsystem or service to which each request is sent. An example of a user request is adding an item to a shopping cart or performing the checkout process in an e-commerce system.

Another common requirement is summarizing performance data in selected percentiles. For example, an operator might determine the response times for 99 percent of requests, 95 percent of requests, and 70 percent of requests. There might be SLA targets or other goals set for each percentile. The ongoing results should be reported in near real time to help detect immediate issues. The results should also be aggregated over the longer time for statistical purposes.

In the case of latency issues affecting performance, an operator should be able to quickly identify the cause of the bottleneck by examining the latency of each step that each request performs. The performance data must therefore provide a means of correlating performance measures for each step to tie them to a specific request.

Depending on the visualization requirements, it might be useful to generate and store a https://en.wikipedia.org/wiki/Data_cube[data cube] that contains views of the raw data. This data cube can allow complex ad hoc querying and analysis of the performance information.

