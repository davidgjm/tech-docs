==== References:

* https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-introduction/4-operational-excellence

=== Questions

* What's analytics?
* What's monitoring?
* instrumentation system

'''

== DevOps

Essential DevOps practices include agile planning, continuous integration, continuous delivery, and monitoring of applications.

The DevOps culture stresses small, multidisciplinary teams that work autonomously and take collective accountability for how end users will experience their software. DevOps teams apply agile practices and include operations in each team's responsibilities. Teams work in small batches that focus on improving the end-to-end delivery of customer value, and they strive to eliminate waste and impediments along the way. There are no silos and no blame games, because each team is mutually accountable.

== Continuous Integration and Continuous Delivery (CI/CD)

Continuous Integration (CI) is the practice of building and testing code every time a team member commits changes to version control. CI encourages developers to share their code and unit tests by merging their changes into a shared version control repository after every small task completion. Committing code triggers an automated build system to grab the latest code from the shared repository, and then build, test, and validate the full master branch.

CI helps developers to identify bugs earlier, and it improves software quality since code is checked in, built, and verified more frequently. Instead of working on code for a month and discovering numerous issues when changes are eventually checked in, developers can check in smaller sets of changes and be confident that their code doesn't introduce large volumes of issues into the master branch.

Continuous Delivery (CD) is the process to build, test, configure and deploy from a build environment to a production environment. Multiple testing or staging environments create a release pipeline to automate the creation of infrastructure and deployment of a new build. Successive environments support progressively longer-running activities of integration, load, and user acceptance testing.

Continuous integration and continuous delivery are often combined into a single pipeline known as CI/CD. Continuous integration starts the continuous delivery process, and the CI/CD pipeline stages changes from each successive environment to the next upon successful completion of the tests that are defined at each stage. As a developer, you can check in code, validate that it passes all tests and introduces no new issues into the master branch, and then roll it out to production with the confidence that it will not impact the operation of your production environment.

== Use monitoring and analytics to gain operational insights

https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-operational-excellence/3-use-monitoring-and-analytics-to-gain-operational-insights

When it comes to monitoring and analytics on Azure, we can bundle services into three specific areas of focus:

* Core monitoring
* Deep infrastructure monitoring
* Deep application monitoring

image::https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-operational-excellence/media/3-monitoring-products-overview.png[image]

=== Core monitoring

Azure provides services to give you visibility into four key core monitoring areas: a**ctivity logging, the health of services, metrics and diagnostics, and recommendations on best practices**.

==== Metrics and diagnostics

Azure Monitor enables core monitoring for Azure services by allowing the collection, aggregation, and visualization of metrics, activity logs, and diagnostic logs.

=== Deep infrastructure monitoring

When you're designing a monitoring strategy, it's important to include every component in the application chain, so you can correlate events across services and resources. Services that support Azure Monitor can be easily configured to send their data to a Log Analytics workspace. Virtual machines (both in the cloud and on-premises) can have an agent installed to send data to Log Analytics. You can submit custom data to Log Analytics through the Log Analytics API. The following illustration shows how Log Analytics acts as a central hub for monitoring data; Log Analytics receives monitoring data from your Azure resources, and makes that data available to consumers for analysis or visualization.

image::https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-operational-excellence/media/3-collecting-data.png[image]

Log Analytics allows you to create queries and interact with other systems based on those queries. The most common example is an alert. Maybe you want to receive an email when a system runs out of disk space, or a best practice on your SQL Servers is no longer being followed. Log Analytics can send alerts, kick off automation, and even hook into custom APIs for things like integration with _IT service management (ITSM)_.

=== Deep application monitoring

Azure Application Insights allows you to do exactly that. Application Insights provides telemetry collection, query, and visualization capabilities. Instrumenting this level of monitoring requires little to no code changes; you only have to install a small instrumentation package into your application. Application Insights is cross platform, supporting .NET, Node.js, or Java.


=== Testing strategies for your application

A main tenet of a DevOps practice to achieve system reliability is the shift left principle. If your process for developing and deploying an application is depicted as a series of steps that are listed from left to right, your testing should be shifted as much as possible toward the beginning of your process (e.g. to the left), and not just at the very end of your process (e.g. to the right). Errors are far cheaper to repair when they are caught early, and issues can be expensive or impossible to fix later in your application life cycle.

Testing should occur on *both application code and infrastructure code*, and they should both be subject to the same quality controls. The environment where applications are running should be version-controlled, and deployed through the same mechanisms as application code. As a result, both application code and infrastructure code can be tested and validated using DevOps testing paradigms.

==== Automated Testing

===== Unit Testing

Unit tests are tests typically run by each new version of code that is committed into your version control system. Unit Tests should be extensive (should cover ideally 100% of the code), and quick (typically under 30 seconds, although this number is not a rule set in stone). Unit testing could verify things like the syntax correctness of application code, Resource Manager templates or Terraform configurations, that the code is following best practices, or that they produce the expected results when provided certain inputs.

Unit tests should be applied both to application code and infrastructure code.

===== Smoke Testing
Smoke tests are more exhaustive than unit tests, but still not as much as integration tests. Smoke tests normally run in less than 15 minutes. While smoke tests do not verify the interoperability of your different components with each other, they verify that each component can be correctly built, and each component meets your criteria for expected functionality and performance.

Smoke tests usually involve building the application code; and if you're deploying infrastructure as part of your process, then possibly testing the deployment in a test environment.

===== Integration Testing
After making sure that your different application components operate correctly individually, integration testing determines whether your components can interact with each other as they should. Integration tests usually take longer than smoke testing; and as a consequence, they are sometimes executed less frequently. For example, running integration tests *every night* still offers a good compromise between the different types of automated testing; your integration testing will detect interoperability issues between application components no later than one day after they were introduced.


==== Manual Testing
===== Acceptance Testing

There are many different ways of confirming that the application is doing what it should.

* **Blue/Green deployments**: when deploying a new application version, you can deploy it in parallel to the existing one. This way you can start redirecting your clients to the new version; if everything goes well, you will decommission the old version. If there is any problem with the new deployment, you can always redirect your clients back to the older deployment.

* **Canary releases**: you can expose new functionality of your application (ideally using feature flags) to a select group of users. If these users are satisfied with the new functionality, you can extend it to the rest of your user community. In this scenario we are talking about releasing functionality, and not necessarily about deploying a new version of the application.

* **A/B testing**: A/B testing is similar to canary release testing, but while canary releases focus on mitigating risk, A/B testing focuses on evaluating the effectiveness of two similar ways of achieving the same goal. For example, if you have two versions of the layout of a certain area of your application, you could send half of your users to one version and the other half your users to the other, and then you could use some metrics to see which layout works better for your application goals.

==== Stress tests
During your stress tests, it is critical that you monitor all the components of the system in order to identify whether there are any scale limitations. Every component of the system that is not able to scale out can turn into a bottleneck (such as active/passive network components or databases). It is important for you to know the limits for each of your components so you can mitigate their impact into your application scale. As you learn more about the performance characteristics for each of your components, the discoveries that you make along the way might motivate you to replace some of your components with more scalable counterparts.

It is equally important to verify that after the stress test is concluded, your infrastructure scales back down to its normal condition in order to keep your costs under control.

==== Fault injection
Your application should be resilient to infrastructure failures, and introducing faults in the underlying infrastructure and observing how your application behaves is fundamental for increasing the trust in your redundancy mechanisms. For example: ungracefully shutting down infrastructure components, degrading the performance of certain elements such as network equipment, or purposely introducing faults in the environment are ways of verifying that your application is going to continue to behave or react as expected, should these situations ever occur in real life.

Most companies use a controlled way of injecting faults into the system; although if your are confident with your application resiliency, you could use automated frameworks. *Chaos engineering* is a practice adopted by some organizations to identify areas where faults may occur by purposefully making key pieces of infrastructure unavailable.


