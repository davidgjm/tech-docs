= Performance Efficiency in Azure
:toc: auto

== Optimize network performance

https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-performance-efficiency/3-optimize-network-performance

In comparison, a cloud environment is built for scale. Cloud-hosted resources might not be in the same rack, datacenter, or region. This distributed approach can have an impact on the round-trip time of your network communications. While all Azure regions are interconnected by a high-speed fiber backbone, the speed of light is still a physical limitation. Calls between services in different physical locations will still have network latency directly correlated to the distance between them.

In addition, depending on the communication needs of an application, more round trips might be required. Each round trip comes with a latency tax, and each round trip adds to the overall latency. The following illustration shows how the latency perceived by the user is the combination of the round trips required to service the request.

image::https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-performance-efficiency/media/3-network-latency.png[Round-trip delay in cloud]

=== Latency between Azure resources
Suppose that your pilot testing of the booking system was successful. As a result, the scope of your pilot test has expanded to include users in Australia. When the users in Australia view your website, they'll incur the additional round-trip time that's necessary to access all of the resources that are located in West Europe. Their experience will be diminished because of the additional network latency.

To address your network latency issues, your IT team decides to host another front-end instance in the Australia East region. This design helps reduce the time for your web servers to return content to users in Australia. But their experience is still diminished because there's significant latency for data that's being transferred between the front-end web servers in Australia East and the database in West Europe.

There are a few ways you could reduce the remaining latency:

* Create a read-replica of the database in Australia East. A read replica allows reads to perform well, but writes still incur latency. Azure SQL Database geo-replication allows for read-replicas.
* Sync your data between regions with Azure SQL Data Sync.
* Use a globally distributed database such as Azure Cosmos DB. This database allows both reads and writes to occur regardless of location. But it might require changes to the way your application stores and references data.
* Use caching technology, such as Azure Cache for Redis, to minimize high-latency calls to remote databases for frequently accessed data.

=== Latency between users and Azure resources

Traffic Manager is a DNS-based load balancer that you can use to distribute traffic within and across Azure regions. Rather than having the user browse to a specific instance of your web front end, Traffic Manager can route users based on a set of characteristics:

* **Priority**: You specify an ordered list of front-end instances. If the one with the highest priority is unavailable, Traffic Manager routes the user to the next available instance.
* **Weighted**: You set a weight against each front-end instance. Traffic Manager then distributes traffic according to those defined ratios.
* **Performance**: Traffic Manager routes users to the closest front-end instance based on network latency.
* **Geographic**: You set up geographical regions for front-end deployments and route your users based on data sovereignty mandates or localization of content.

.Note
[IMPORTANT]
====
It's important to note that this load balancing (Traffic Manager) is only handled via DNS. No inline load balancing or caching is happening here. Traffic Manager simply returns the DNS name of the closest front end to the user.
====

=== Use a CDN to cache content close to users

Web static content (__web pages, images, videos, etc.__) can be delivered to users faster by using a content delivery network (**CDN**), such as Azure Content Delivery Network.

This approach puts content closer to the destination, which reduces latency and improves the user experience. The following illustration shows how using Azure Content Delivery Network puts content closer to the destination, which reduces latency and improves the user experience.

image::https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-performance-efficiency/media/3-cdn.png[]

Content delivery networks can also be used to host cached dynamic content. Extra consideration is required though, because cached content might be out of date compared with the source. Context expiration can be controlled by setting a time to live (TTL). If the TTL is too high, out-of-date content might be displayed and the cache would need to be purged.

One way to handle cached content is with a feature called dynamic site acceleration, which can increase performance of webpages with dynamic content. Dynamic site acceleration can also provide a low-latency path to additional services in your solution. An example is an API endpoint.

== Optimize storage performance

=== Optimize virtual machine storage performance
Different applications have different storage requirements. Your application might be sensitive to latency of disk reads and writes. Or, it might require the ability to handle a large number of input/output operations per second (IOPS) or greater overall disk throughput.

Disks can be striped by using a striping technology like Storage Spaces on Windows or mdadm on Linux. Striping increases the throughput and IOPS by spreading disk activity across multiple disks. You can use disk striping to push the limits of performance for disks. Striping is often seen in high-performance database systems and other systems with intensive storage requirements.

=== Optimize storage performance for your application

==== Polyglot persistence
Polyglot persistence is the use of different data storage technologies to handle your storage requirements.
image::https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-performance-efficiency/media/4-polyglot-persistence.png[]

*It's important to know that different data stores are designed for certain use cases or might be more accessible because of cost. As an example, storing blobs in a SQL database might be costly and slower to access than directly from a blob store.*

Maintaining data consistency across distributed data stores can be a significant challenge. The issue is that strategies such as serialization and locking only work well if all application instances share the same data store and the application is designed to ensure that the locks are short lived. But if data is partitioned or replicated across different data stores, locking and serializing data access to maintain consistency can become an expensive overhead that affects the throughput, response time, and scalability of a system. As a result, most modern distributed applications don't lock the data that they modify. They take a more relaxed approach to consistency, which is known as eventual consistency.

Eventual consistency means that replica data stores eventually converge if there are no further writes. If a write is made to one of the data stores, reads from another data store might provide slightly out-of-date data. Eventual consistency enables higher scale because there's a low latency for reads and writes, instead of waiting to check if information is consistent across all stores.

== Identify performance bottlenecks in your application
=== Performance requirements

Nonfunctional requirements help you find that point. These particular requirements don't tell you what your app must do. Instead, they tell you what quality levels it must meet. For example, you can define nonfunctional requirements to discover:

* How fast a transaction must return under a given load.
* How many simultaneous connections your application needs to support before it begins to return errors.
* If there's server failure, what's the maximum amount of time your application is allowed to be down before a backup is online.

Defining these requirements in advance of building your solution is critical. They ensure that your application meets expectations but doesn't require more effort or cost more money than necessary. You can also plan your monitoring and operations rules around these nonfunctional requirements.

You should discuss requirements with your stakeholders or customers, document them, and communicate them broadly to ensure that everyone agrees on what good performance means.

=== Performance monitoring options in Azure

==== Azure Monitor

Azure Monitor provides a single management point for infrastructure-level logs and monitoring for most of your Azure services. The following diagram depicts a high-level view of Azure Monitor. At the center of the diagram are the data stores for *metrics and logs*. These are the two fundamental types of data that Azure Monitor uses. On the left side are the sources of monitoring data that populate these data stores. On the right side are the different functions that Azure Monitor performs with this collected data, such as analysis, alerting, and streaming to external systems.
image::https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-performance-efficiency/media/5-azure-monitor.png[]

Azure Monitor can collect data from a variety of sources. You can think of monitoring data for your applications as occurring in tiers that range from your application to any OS and the services it relies on to the platform itself. Azure Monitor collects data from each of the following tiers:

* Application monitoring data: Data about the performance and functionality of the code you've written, regardless of its platform.
* Guest OS monitoring data: Data about the OS on which your application is running. This OS might be in Azure, another cloud, or on-premises.
* Azure resource monitoring data: Data about the operation of an Azure resource.
* Azure subscription monitoring data: Data about the operation and management of an Azure subscription, and data about the health and operation of Azure itself.
* Azure tenant monitoring data: Data about the operation of tenant-level Azure services, such as Azure Active Directory (Azure AD).

==== Log Analytics
image::https://docs.microsoft.com/en-us/learn/modules/azure-well-architected-performance-efficiency/media/5-log-analytics.png[]

You can collate a wide range of data sources, security logs, Azure activity logs, and server, network, and application logs. You can also push on-premises System Center Operations Manager data to Log Analytics in hybrid deployment scenarios. Then Azure SQL Database can send diagnostic information directly into Log Analytics for detailed performance monitoring.

Centralized logging can be highly beneficial for troubleshooting all types of scenarios. You can use it to troubleshoot performance issues. Centralized logging is a key part of a good monitoring strategy for any architecture.

=== Application performance management

Deep application issues are often tricky to track down. This type of scenario is when it can be beneficial to integrate telemetry into your application by using an application performance management (APM) solution. An APM solution helps you to track down low-level application performance and behavior. Telemetry can include individual page request times, exceptions within your application, and even custom metrics to track business logic. This telemetry can provide a wealth of insight into what's going on within your application.

Application Insights is an Azure service that provides this deep application performance management. You install a small instrumentation package in your application and then set up an Application Insights resource in the Azure portal. The instrumentation monitors your app and sends telemetry data to the portal.

You can use Application Insights to consume telemetry from the host environments, such as performance counters, Azure diagnostics, and Docker logs. You can also set up web tests that periodically send synthetic requests to your web service. You could even configure your application to send custom events and metrics that you write yourself in the client or server code. For example, you can track application-specific events such as items sold or games won.

_Application Insights stores its data in a common repository_, and metrics are shared with Azure Monitor. Application Insights can also take advantage of shared functionality such as alerts, dashboards, and deep analysis with the Log Analytics query language.

A common pattern used to determine the availability of a web application is the health endpoint monitoring pattern. This pattern is used to monitor web applications and their associated back-end services to ensure that they're available and performing correctly. The pattern is implemented by querying a particular URI. The endpoint checks on the status of many components. Even the back-end services that the app depends on are checked instead of only the availability of the front end itself. The health endpoint monitoring pattern acts as a service-level health check that returns an indication of the overall health of the service.

You should use an APM solution such as Application Insights to gain a deep understanding of your application and to correlate activity across your application. An APM solution can help you understand how a specific action works in the client browser, on the server, and through to downstream services. It also provides insight into trends. An APM solution provides notifications when there's a problem, helps to identify where the problem is, and informs you on how to fix it before your users are aware of it.

