<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<title>Monitoring and diagnostics</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/darshandsoni/asciidoctor-skins/css/material-teal.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="./rouge-github.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>Monitoring and diagnostics</h1>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_monitoring_and_diagnostics_scenarios">Monitoring and diagnostics scenarios</a></li>
<li><a href="#_health_monitoring">Health monitoring</a>
<ul class="sectlevel2">
<li><a href="#_requirements_for_health_monitoring">Requirements for health monitoring</a></li>
<li><a href="#_data_sources_instrumentation_and_data_collection_requirements">Data sources, instrumentation, and data-collection requirements</a></li>
<li><a href="#_analyzing_health_data">Analyzing health data</a></li>
</ul>
</li>
<li><a href="#_availability_monitoring">Availability monitoring</a>
<ul class="sectlevel2">
<li><a href="#_requirements_for_availability_monitoring">Requirements for availability monitoring</a></li>
<li><a href="#_data_sources_instrumentation_and_data_collection_requirements_2">Data sources, instrumentation, and data-collection requirements</a></li>
<li><a href="#_analyzing_availability_data">Analyzing availability data</a></li>
</ul>
</li>
<li><a href="#_performance_monitoring">Performance monitoring</a>
<ul class="sectlevel2">
<li><a href="#_requirements_for_performance_monitoring">Requirements for performance monitoring</a></li>
<li><a href="#_data_sources_instrumentation_and_data_collection_requirements_3">Data sources, instrumentation, and data-collection requirements</a></li>
<li><a href="#_analyzing_performance_data">Analyzing performance data</a></li>
</ul>
</li>
<li><a href="#_security_monitoring">Security Monitoring</a>
<ul class="sectlevel2">
<li><a href="#_requirements_for_security_monitoring">Requirements for security monitoring</a></li>
<li><a href="#_data_sources_instrumentation_and_data_collection_requirements_4">Data sources, instrumentation, and data-collection requirements</a></li>
<li><a href="#_analyzing_security_data">Analyzing security data</a></li>
</ul>
</li>
<li><a href="#_sla-monitoring">SLA monitoring</a>
<ul class="sectlevel2">
<li><a href="#_requirements_for_sla_monitoring">Requirements for SLA monitoring</a></li>
<li><a href="#_data_sources_instrumentation_and_data_collection_requirements_5">Data sources, instrumentation, and data-collection requirements</a></li>
<li><a href="#_analyzing_sla_data">Analyzing SLA data</a></li>
</ul>
</li>
<li><a href="#_auditing">Auditing</a>
<ul class="sectlevel2">
<li><a href="#_requirements_for_auditing">Requirements for auditing</a></li>
<li><a href="#_data_sources_instrumentation_and_data_collection_requirements_6">Data sources, instrumentation, and data-collection requirements</a></li>
<li><a href="#_analyzing_audit_data">Analyzing audit data</a></li>
</ul>
</li>
<li><a href="#_the_monitoring_and_diagnostics_pipeline">The monitoring and diagnostics pipeline</a></li>
<li><a href="#_sources_of_monitoring_and_diagnostic_data">Sources of monitoring and diagnostic data</a></li>
<li><a href="#_instrumenting_an_application">Instrumenting an application</a>
<ul class="sectlevel2">
<li><a href="#_information_for_correlating_data">Information for correlating data</a></li>
<li><a href="#_information_to_include_in_the_instrumentation_data">Information to include in the instrumentation data</a></li>
<li><a href="#_ensuring_compatibility_with_telemetry_systems">Ensuring compatibility with telemetry systems</a></li>
<li><a href="#_best_practices_for_instrumenting_applications">Best practices for instrumenting applications</a></li>
</ul>
</li>
<li><a href="#_collecting_and_storing_data">Collecting and storing data</a>
<ul class="sectlevel2">
<li><a href="#_strategies_for_collecting_instrumentation_data">Strategies for collecting instrumentation data</a></li>
</ul>
</li>
<li><a href="#_analyzing_data_and_diagnosing_issues">Analyzing data and diagnosing issues</a>
<ul class="sectlevel2">
<li><a href="#_supporting_hot_warm_and_cold_analysis">Supporting hot, warm, and cold analysis</a></li>
<li><a href="#_correlating_data">Correlating data</a></li>
<li><a href="#_troubleshooting_and_diagnosing_issues">Troubleshooting and diagnosing issues</a></li>
</ul>
</li>
<li><a href="#_visualization_by_using_dashboards">Visualization by using dashboards</a>
<ul class="sectlevel2">
<li><a href="#_raising_alerts">Raising alerts</a></li>
<li><a href="#_reporting">Reporting</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="sidebarblock">
<div class="content">
<div class="title">References</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://docs.microsoft.com/en-us/azure/architecture/best-practices/monitoring">Best practices for monitoring cloud applications</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="ðŸ’¡"></i>
</td>
<td class="content">
<div class="title">Reflections</div>
<div class="ulist">
<ul>
<li>
<p>How can this be implemented with Spring Boot Actuator endpoints?</p>
</li>
<li>
<p>Which Actuator endpoints correspond to:</p>
<div class="ulist">
<ul>
<li>
<p>health monitoring</p>
</li>
<li>
<p>availability monitoring</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_monitoring_and_diagnostics_scenarios">Monitoring and diagnostics scenarios</h2>
<div class="sectionbody">
<div class="paragraph">
<p><span class="icon"><i class="fa fa-android"></i></span></p>
</div>
<div class="paragraph">
<p>You can use monitoring to gain an insight into how well a system is functioning. Monitoring is a crucial part of maintaining quality-of-service targets. Common scenarios for collecting monitoring data include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Ensuring that the system remains healthy.</p>
</li>
<li>
<p>Tracking the availability of the system and its component elements.</p>
</li>
<li>
<p>Maintaining performance to ensure that the throughput of the system does not degrade unexpectedly as the volume of work increases.</p>
</li>
<li>
<p>Guaranteeing that the system meets any service-level agreements (SLAs) established with customers.</p>
</li>
<li>
<p>Protecting the privacy and security of the system, users, and their data.</p>
</li>
<li>
<p>Tracking the operations that are performed for auditing or regulatory purposes.</p>
</li>
<li>
<p>Monitoring the day-to-day usage of the system and spotting trends that might lead to problems if they&#8217;re not addressed.</p>
</li>
<li>
<p>Tracking issues that occur, from initial report through to analysis of possible causes, rectification, consequent software updates, and deployment.</p>
</li>
<li>
<p>Tracing operations and debugging software releases.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This list is not intended to be comprehensive. This document focuses on these scenarios as the most common situations for performing monitoring. There might be others that are less common or are specific to your environment.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_health_monitoring">Health monitoring</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The purpose of health monitoring is to generate a snapshot of the current health of the system so that you can verify that all components of the system are functioning as expected.</p>
</div>
<div class="sect2">
<h3 id="_requirements_for_health_monitoring">Requirements for health monitoring</h3>
<div class="paragraph">
<p>An operator should be alerted quickly (within a matter of seconds) if any part of the system is deemed to be unhealthy. The operator should be able to ascertain which parts of the system are functioning normally, and which parts are experiencing problems. System health can be highlighted through a <em>traffic-light system</em>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Red</strong> for unhealthy (the system has stopped)</p>
</li>
<li>
<p><strong>Yellow</strong> for partially healthy (the system is running with reduced functionality)</p>
</li>
<li>
<p><strong>Green</strong> for completely healthy</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A comprehensive health-monitoring system enables an operator to drill down through the system to view the health status of subsystems and components. For example, if the overall system is depicted as partially healthy, the operator should be able to zoom in and determine which functionality is currently unavailable.</p>
</div>
</div>
<div class="sect2">
<h3 id="_data_sources_instrumentation_and_data_collection_requirements">Data sources, instrumentation, and data-collection requirements</h3>
<div class="paragraph">
<p>The raw data that&#8217;s required to support health monitoring can be generated as a result of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Tracing execution of user requests. This information can be used to determine which requests have succeeded, which have failed, and how long each request takes.</p>
</li>
<li>
<p>Synthetic user monitoring. This process simulates the steps performed by a user and follows a predefined series of steps. The results of each step should be captured.</p>
</li>
<li>
<p>Logging exceptions, faults, and warnings. This information can be captured as a result of trace statements embedded into the application code, as well as retrieving information from the event logs of any services that the system references.</p>
</li>
<li>
<p>Monitoring the health of any third-party services that the system uses. This monitoring might require retrieving and parsing health data that these services supply. This information might take a variety of formats.</p>
</li>
<li>
<p>Endpoint monitoring. This mechanism is described in more detail in the "Availability monitoring" <a href="#_availability_monitoring">Availability monitoring</a>  section.</p>
</li>
<li>
<p>Collecting ambient performance information, such as background CPU utilization or I/O (including network) activity.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_analyzing_health_data">Analyzing health data</h3>
<div class="paragraph">
<p><mark>The primary focus of health monitoring is to quickly indicate whether the system is running.</mark> Hot analysis of the immediate data can trigger an alert if a critical component is detected as unhealthy. (It fails to respond to a consecutive series of pings, for example.) The operator can then take the appropriate corrective action.</p>
</div>
<div class="paragraph">
<p>A more advanced system might include a predictive element that performs a cold analysis over recent and current workloads. A cold analysis can spot trends and determine whether the system is likely to remain healthy or whether the system will need additional resources. This predictive element should be based on critical performance metrics, such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rate of requests directed at each service or subsystem.</p>
</li>
<li>
<p>The response times of these requests.</p>
</li>
<li>
<p>The volume of data flowing into and out of each service.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If the value of any metric exceeds a defined threshold, the system can raise an alert to enable an operator or autoscaling (if available) to take the preventative actions necessary to maintain system health. These actions might involve adding resources, restarting one or more services that are failing, or applying throttling to lower-priority requests.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_availability_monitoring">Availability monitoring</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A truly healthy system requires that the components and subsystems that compose the system are available. Availability monitoring is closely related to health monitoring. But whereas health monitoring provides an immediate view of the current health of the system, <mark>availability monitoring is concerned with tracking the availability of the system and its components to generate statistics about the uptime of the system.</mark></p>
</div>
<div class="paragraph">
<p>In many systems, some components (such as a database) are configured with built-in redundancy to permit rapid failover in the event of a serious fault or loss of connectivity. Ideally, users should not be aware that such a failure has occurred. But from an availability monitoring perspective, it&#8217;s necessary to gather as much information as possible about such failures to determine the cause and take corrective actions to prevent them from recurring.</p>
</div>
<div class="paragraph">
<p>The data that&#8217;s required to track availability might depend on a number of lower-level factors. Many of these factors might be specific to the application, system, and environment. An effective monitoring system captures the availability data that corresponds to these low-level factors and then aggregates them to give an overall picture of the system. For example, in an e-commerce system, the business functionality that enables a customer to place orders might depend on the repository where order details are stored and the payment system that handles the monetary transactions for paying for these orders. The availability of the order-placement part of the system is therefore a function of the availability of the repository and the payment subsystem.</p>
</div>
<div class="sect2">
<h3 id="_requirements_for_availability_monitoring">Requirements for availability monitoring</h3>
<div class="paragraph">
<p>An operator should also be able to view the historical availability of each system and subsystem, and use this information to spot any trends that might cause one or more subsystems to periodically fail. (Do services start to fail at a particular time of day that corresponds to peak processing hours?)</p>
</div>
<div class="paragraph">
<p>A monitoring solution should provide an immediate and historical view of the availability or unavailability of each subsystem. It should also be capable of quickly alerting an operator when one or more services fail or when users can&#8217;t connect to services. This is a matter of not only monitoring each service, but also examining the actions that each user performs if these actions fail when they attempt to communicate with a service. To some extent, a degree of connectivity failure is normal and might be due to transient errors. But it might be useful to allow the system to raise an alert for the number of connectivity failures to a specified subsystem that occur during a specific period.</p>
</div>
</div>
<div class="sect2">
<h3 id="_data_sources_instrumentation_and_data_collection_requirements_2">Data sources, instrumentation, and data-collection requirements</h3>
<div class="paragraph">
<p>As with health monitoring, the raw data that&#8217;s required to support availability monitoring can be generated as a result of synthetic user monitoring and logging any exceptions, faults, and warnings that might occur. In addition, availability data can be obtained from performing endpoint monitoring. The application can expose one or more health endpoints, each testing access to a functional area within the system. The monitoring system can ping each endpoint by following a defined schedule and collect the results (success or fail).</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
All timeouts, network connectivity failures, and connection retry attempts must be recorded. All data should be timestamped.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_analyzing_availability_data">Analyzing availability data</h3>
<div class="paragraph">
<p>The instrumentation data must be aggregated and correlated to support the following types of analysis:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The immediate availability of the system and subsystems.</p>
</li>
<li>
<p>The availability failure rates of the system and subsystems. Ideally, an operator should be able to correlate failures with specific activities: what was happening when the system failed?</p>
</li>
<li>
<p>A historical view of failure rates of the system or any subsystems across any specified period, and the load on the system (number of user requests, for example) when a failure occurred.</p>
</li>
<li>
<p>The reasons for unavailability of the system or any subsystems. For example, the reasons might be service not running, connectivity lost, connected but timing out, and connected but returning errors.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can calculate the percentage availability of a service over a period of time by using the following formula:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="console"><span class="go">%Availability =  ((Total Time â€“ Total Downtime) / Total Time ) * 100</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>This is useful for SLA purposes. (<a href="#_sla-monitoring">SLA monitoring</a> is described in more detail later in this guidance.) The definition of <em>downtime</em> depends on the service. For example, Visual Studio Team Services Build Service defines downtime as the period (total accumulated minutes) during which Build Service is unavailable. A minute is considered unavailable if all continuous HTTP requests to Build Service to perform customer-initiated operations throughout the minute either result in an error code or do not return a response.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_performance_monitoring">Performance monitoring</h2>
<div class="sectionbody">
<div class="sidebarblock">
<div class="content">
<div class="title"><span class="icon lime"><i class="fa fa-sticky-note fa-2x"></i></span> Open Questions</div>
<div class="ulist">
<ul>
<li>
<p>How to achieve this with Spring Cloud/Spring Boot on Kubernetes in Azure?</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>System performance depends on a number of factors. Each factor is typically measured through key performance indicators (KPIs), such as the number of database transactions per second or the volume of network requests that are successfully serviced in a specified time frame. Some of these KPIs might be available as specific performance measures, whereas others might be derived from a combination of metrics.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Determining poor or good performance requires that you understand the level of performance at which the system should be capable of running. This requires observing the system while it&#8217;s functioning under a typical load and capturing the data for each KPI over a period of time. This might involve running the system under a simulated load in a test environment and gathering the appropriate data before deploying the system to a production environment.</p>
</div>
<div class="paragraph">
<p>You should also ensure that monitoring for performance purposes does not become a burden on the system. You might be able to dynamically adjust the level of detail for the data that the performance monitoring process gathers.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_requirements_for_performance_monitoring">Requirements for performance monitoring</h3>
<div class="paragraph">
<p>To examine system performance, an operator typically needs to see information that includes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The response rates for user requests.</p>
</li>
<li>
<p>The number of concurrent user requests.</p>
</li>
<li>
<p>The volume of network traffic.</p>
</li>
<li>
<p>The rates at which business transactions are being completed.</p>
</li>
<li>
<p>The average processing time for requests.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>It can also be helpful to provide tools that enable an operator to help spot correlations, such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The number of concurrent users versus request latency times (how long it takes to start processing a request after the user has sent it).</p>
</li>
<li>
<p>The number of concurrent users versus the average response time (how long it takes to complete a request after it has started processing).</p>
</li>
<li>
<p>The volume of requests versus the number of processing errors.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Along with this high-level functional information, an operator should be able to obtain a detailed view of the performance for <strong>each component</strong> in the system. This data is typically provided through low-level performance counters that track information such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Memory utilization.</p>
</li>
<li>
<p>Number of threads.</p>
</li>
<li>
<p>CPU processing time.</p>
</li>
<li>
<p>Request queue length.</p>
</li>
<li>
<p>Disk or network I/O rates and errors.</p>
</li>
<li>
<p>Number of bytes written or read.</p>
</li>
<li>
<p>Middleware indicators, such as queue length.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All visualizations should allow an operator to specify a time period. The displayed data might be a snapshot of the current situation and/or a historical view of the performance.</p>
</div>
<div class="paragraph">
<p>An operator should be able to raise an alert based on any performance measure for any specified value during any specified time interval.</p>
</div>
</div>
<div class="sect2">
<h3 id="_data_sources_instrumentation_and_data_collection_requirements_3">Data sources, instrumentation, and data-collection requirements</h3>
<div class="paragraph">
<p>You can gather high-level performance data (throughput, number of concurrent users, number of business transactions, error rates, and so on) by monitoring the progress of users' requests as they arrive and pass through the system. This involves incorporating tracing statements at key points in the application code, together with timing information. All faults, exceptions, and warnings should be captured with sufficient data for correlating them with the requests that caused them.</p>
</div>
<div class="paragraph">
<p>If possible, you should also capture performance data for any external systems that the application uses. These external systems might provide their own performance counters or other features for requesting performance data. If this is not possible, record information such as the start time and end time of each request made to an external system, together with the status (success, fail, or warning) of the operation. For example, you can use a stopwatch approach to time requests: start a timer when the request starts and then stop the timer when the request finishes.</p>
</div>
<div class="paragraph">
<p>Low-level performance data for individual components in a system might be available through features and services such as Windows performance counters and Azure Diagnostics.</p>
</div>
</div>
<div class="sect2">
<h3 id="_analyzing_performance_data">Analyzing performance data</h3>
<div class="paragraph">
<p>Much of the analysis work consists of aggregating performance data by user request type and/or the subsystem or service to which each request is sent. An example of a user request is adding an item to a shopping cart or performing the checkout process in an e-commerce system.</p>
</div>
<div class="paragraph">
<p>Another common requirement is summarizing performance data in selected percentiles. For example, an operator might determine the response times for 99 percent of requests, 95 percent of requests, and 70 percent of requests. There might be SLA targets or other goals set for each percentile. The ongoing results should be reported in near real time to help detect immediate issues. The results should also be aggregated over the longer time for statistical purposes.</p>
</div>
<div class="paragraph">
<p>In the case of latency issues affecting performance, an operator should be able to quickly identify the cause of the bottleneck by examining the latency of each step that each request performs. The performance data must therefore provide a means of correlating performance measures for each step to tie them to a specific request.</p>
</div>
<div class="paragraph">
<p>Depending on the visualization requirements, it might be useful to generate and store a <a href="https://en.wikipedia.org/wiki/Data_cube">data cube</a> that contains views of the raw data. This data cube can allow complex ad hoc querying and analysis of the performance information.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_security_monitoring">Security Monitoring</h2>
<div class="sectionbody">
<div class="paragraph">
<p>All commercial systems that include sensitive data must implement a security structure. The complexity of the security mechanism is usually a function of the sensitivity of the data. In a system that requires users to be authenticated, you should record:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>All sign-in attempts, whether they fail or succeed.</p>
</li>
<li>
<p>All operations performed byâ€”and the details of all resources accessed byâ€”an authenticated user.</p>
</li>
<li>
<p>When a user ends a session and signs out.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Monitoring might be able to help detect attacks on the system. For example, a large number of failed sign-in attempts might indicate a brute-force attack. An unexpected surge in requests might be the result of a distributed denial-of-service (DDoS) attack. <strong>You must be prepared to monitor all requests to all resources <mark>regardless of the source of these requests</mark>.</strong> A system that has a sign-in vulnerability might accidentally expose resources to the outside world without requiring a user to actually sign in.</p>
</div>
<div class="sect2">
<h3 id="_requirements_for_security_monitoring">Requirements for security monitoring</h3>
<div class="paragraph">
<p>The most critical aspects of security monitoring should enable an operator to quickly:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Detect attempted intrusions by an unauthenticated entity.</p>
</li>
<li>
<p>Identify attempts by entities to perform operations on data for which they have not been granted access.</p>
</li>
<li>
<p>Determine whether the system, or some part of the system, is under attack from outside or inside. (For example, a malicious authenticated user might be attempting to bring the system down.)</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>To support these requirements, an operator should be notified if:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>One account makes repeated failed sign-in attempts within a specified period.</p>
</li>
<li>
<p>One authenticated account repeatedly tries to access a prohibited resource during a specified period.</p>
</li>
<li>
<p>A large number of unauthenticated or unauthorized requests occur during a specified period.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The information that&#8217;s provided to an operator should include the host address of the source for each request. If security violations regularly arise from a particular range of addresses, these hosts might be blocked.</p>
</div>
<div class="paragraph">
<p>A key part in maintaining the security of a system is being able to quickly detect actions that deviate from the usual pattern. Information such as the number of failed and/or successful sign-in requests can be displayed visually to help detect whether there is a spike in activity at an unusual time. (An example of this activity is users signing in at 3:00 AM and performing a large number of operations when their working day starts at 9:00 AM). This information can also be used to help configure time-based autoscaling. For example, if an operator observes that a large number of users regularly sign in at a particular time of day, the operator can arrange to start additional authentication services to handle the volume of work, and then shut down these additional services when the peak has passed.</p>
</div>
</div>
<div class="sect2">
<h3 id="_data_sources_instrumentation_and_data_collection_requirements_4">Data sources, instrumentation, and data-collection requirements</h3>
<div class="paragraph">
<p>Security is an all-encompassing aspect of most distributed systems. The pertinent data is likely to be generated at multiple points throughout a system. You should consider adopting a <em>Security Information and Event Management (SIEM)</em> approach to gather the security-related information that results from events raised by the application, network equipment, servers, firewalls, antivirus software, and other intrusion-prevention elements.</p>
</div>
<div class="paragraph">
<p>Security monitoring can incorporate data from tools that are not part of your application. These tools can include utilities that identify port-scanning activities by external agencies, or network filters that detect attempts to gain unauthenticated access to your application and data.</p>
</div>
<div class="paragraph">
<p>In all cases, the gathered data must enable an administrator to determine the nature of any attack and take the appropriate countermeasures.</p>
</div>
</div>
<div class="sect2">
<h3 id="_analyzing_security_data">Analyzing security data</h3>
<div class="paragraph">
<p>A feature of security monitoring is the variety of sources from which the data arises. The different formats and level of detail often require complex analysis of the captured data to tie it together into a coherent thread of information. Apart from the simplest of cases (such as detecting a large number of failed sign-ins, or repeated attempts to gain unauthorized access to critical resources), it might not be possible to perform any complex automated processing of security data. Instead, it might be preferable to write this data, timestamped but otherwise in its original form, to a secure repository to allow for expert manual analysis.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sla-monitoring">SLA monitoring</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Many commercial systems that support paying customers make guarantees about the performance of the system in the form of SLAs. Essentially, SLAs state that the system can handle a defined volume of work within an agreed time frame and without losing critical information. SLA monitoring is concerned with ensuring that the system can meet measurable SLAs.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
SLA monitoring is closely related to performance monitoring. But whereas performance monitoring is concerned with ensuring that the system functions <em>optimally</em>, SLA monitoring is governed by a contractual obligation that defines what <em>optimally</em> actually means.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>SLAs are often defined in terms of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Overall system availability. For example, an organization might guarantee that the system will be available for 99.9 percent of the time. This equates to no more than 9 hours of downtime per year, or approximately 10 minutes a week.</p>
</li>
<li>
<p>Operational throughput. This aspect is often expressed as one or more high-water marks, such as guaranteeing that the system can support up to 100,000 concurrent user requests or handle 10,000 concurrent business transactions.</p>
</li>
<li>
<p>Operational response time. The system might also make guarantees for the rate at which requests are processed. An example is that 99 percent of all business transactions will finish within 2 seconds, and no single transaction will take longer than 10 seconds.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Some contracts for commercial systems might also include SLAs for customer support. An example is that all help-desk requests will elicit a response within five minutes, and that 99 percent of all problems will be fully addressed within 1 working day. Effective issue tracking (described later in this section) is key to meeting SLAs such as these.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_requirements_for_sla_monitoring">Requirements for SLA monitoring</h3>
<div class="paragraph">
<p>At the highest level, an operator should be able to determine at a glance whether the system is meeting the agreed SLAs or not. And if not, the operator should be able to drill down and examine the underlying factors to determine the reasons for substandard performance.</p>
</div>
<div class="paragraph">
<p>Typical high-level indicators that can be depicted visually include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The percentage of service uptime.</p>
</li>
<li>
<p>The application throughput (measured in terms of successful transactions and/or operations per second).</p>
</li>
<li>
<p>The number of successful/failing application requests.</p>
</li>
<li>
<p>The number of application and system faults, exceptions, and warnings.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All of these indicators should be capable of being filtered by a specified period of time.</p>
</div>
<div class="paragraph">
<p>A cloud application will likely comprise a number of subsystems and components. An operator should be able to select a high-level indicator and see how it&#8217;s composed from the health of the underlying elements. For example, if the uptime of the overall system falls below an acceptable value, an operator should be able to zoom in and determine which elements are contributing to this failure.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>System uptime needs to be defined carefully. In a system that uses redundancy to ensure maximum availability, individual instances of elements might fail, but the system can remain functional. System uptime as presented by health monitoring should indicate the aggregate uptime of each element and not necessarily whether the system has actually halted. Additionally, failures might be isolated. So even if a specific system is unavailable, the remainder of the system might remain available, although with decreased functionality. (In an e-commerce system, a failure in the system might prevent a customer from placing orders, but the customer might still be able to browse the product catalog.)</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For alerting purposes, the system should be able to raise an event if any of the high-level indicators exceed a specified threshold. The lower-level details of the various factors that compose the high-level indicator should be available as contextual data to the alerting system.</p>
</div>
</div>
<div class="sect2">
<h3 id="_data_sources_instrumentation_and_data_collection_requirements_5">Data sources, instrumentation, and data-collection requirements</h3>
<div class="paragraph">
<p>The raw data that&#8217;s required to support SLA monitoring is similar to the raw data that&#8217;s required for performance monitoring, together with some aspects of health and availability monitoring. (See those sections for more details.) You can capture this data by:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Performing endpoint monitoring.</p>
</li>
<li>
<p>Logging exceptions, faults, and warnings.</p>
</li>
<li>
<p>Tracing the execution of user requests.</p>
</li>
<li>
<p>Monitoring the availability of any third-party services that the system uses.</p>
</li>
<li>
<p>Using performance metrics and counters.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All data must be timed and timestamped.</p>
</div>
</div>
<div class="sect2">
<h3 id="_analyzing_sla_data">Analyzing SLA data</h3>
<div class="paragraph">
<p>The instrumentation data must be aggregated to generate a picture of the overall performance of the system. Aggregated data must also support drill-down to enable examination of the performance of the underlying subsystems. For example, you should be able to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Calculate the total number of user requests during a specified period and determine the success and failure rate of these requests.</p>
</li>
<li>
<p>Combine the response times of user requests to generate an overall view of system response times.</p>
</li>
<li>
<p>Analyze the progress of user requests to break down the overall response time of a request into the response times of the individual work items in that request.</p>
</li>
<li>
<p>Determine the overall availability of the system as a percentage of uptime for any specific period.</p>
</li>
<li>
<p>Analyze the percentage time availability of the individual components and services in the system. This might involve parsing logs that third-party services have generated.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Many commercial systems are required to report real performance figures against agreed SLAs for a specified period, typically a month. This information can be used to calculate credits or other forms of repayments for customers if the SLAs are not met during that period. You can calculate availability for a service by using the technique described in the section Analyzing availability data.</p>
</div>
<div class="paragraph">
<p>For internal purposes, an organization might also track the number and nature of incidents that caused services to fail. Learning how to resolve these issues quickly, or eliminate them completely, will help to reduce downtime and meet SLAs.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_auditing">Auditing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Depending on the nature of the application, there might be statutory or other legal regulations that specify requirements for auditing users' operations and recording all data access. Auditing can provide evidence that links customers to specific requests. Nonrepudiation is an important factor in many e-business systems to help maintain trust be between a customer and the organization that&#8217;s responsible for the application or service.</p>
</div>
<div class="sect2">
<h3 id="_requirements_for_auditing">Requirements for auditing</h3>
<div class="paragraph">
<p>An analyst must be able to trace the sequence of business operations that users are performing so that you can reconstruct users' actions. This might be necessary simply as a matter of record, or as part of a forensic investigation.</p>
</div>
<div class="paragraph">
<p>Audit information is highly sensitive. It will likely include data that identifies the users of the system, together with the tasks that they&#8217;re performing. For this reason, audit information will most likely take the form of reports that are available only to trusted analysts rather than as an interactive system that supports drill-down of graphical operations. An analyst should be able to generate a range of reports. For example, reports might list all users' activities occurring during a specified time frame, detail the chronology of activity for a single user, or list the sequence of operations performed against one or more resources.</p>
</div>
</div>
<div class="sect2">
<h3 id="_data_sources_instrumentation_and_data_collection_requirements_6">Data sources, instrumentation, and data-collection requirements</h3>
<div class="paragraph">
<p>The primary sources of information for auditing can include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The security system that manages user authentication.</p>
</li>
<li>
<p>Trace logs that record user activity.</p>
</li>
<li>
<p>Security logs that track all identifiable and unidentifiable network requests.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The format of the audit data and the way in which it&#8217;s stored might be driven by regulatory requirements. For example, it might not be possible to clean the data in any way. (It must be recorded in its original format.) Access to the repository where it&#8217;s held must be protected to prevent tampering.</p>
</div>
</div>
<div class="sect2">
<h3 id="_analyzing_audit_data">Analyzing audit data</h3>
<div class="paragraph">
<p>An analyst must be able to access the raw data in its entirety, in its original form. Aside from the requirement to generate common audit reports, the tools for analyzing this data are likely to be specialized and kept external to the system.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_monitoring_and_diagnostics_pipeline">The monitoring and diagnostics pipeline</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Monitoring a large-scale distributed system poses a significant challenge. Each of the scenarios described in the previous section should not necessarily be considered in isolation. There is likely to be a significant overlap in the monitoring and diagnostic data that&#8217;s required for each situation, although this data might need to be processed and presented in different ways. For these reasons, you should take a holistic view of monitoring and diagnostics.</p>
</div>
<div class="paragraph">
<p>You can envisage the entire monitoring and diagnostics process as a pipeline that comprises the stages shown in Figure 1.</p>
</div>
<div id="img-pipeline" class="imageblock">
<div class="content">
<img src="./images/pipeline.png" alt="The stages in the monitoring and diagnostics pipeline.">
</div>
<div class="title">Figure 1. The stages in the monitoring and diagnostics pipeline</div>
</div>
<div class="paragraph">
<p>Figure 1 highlights how the data for monitoring and diagnostics can come from a variety of data sources. The instrumentation and collection stages are concerned with identifying the sources from where the data needs to be captured, determining which data to capture, how to capture it, and how to format this data so that it can be easily examined. The analysis/diagnosis stage takes the raw data and uses it to generate meaningful information that an operator can use to determine the state of the system. The operator can use this information to make decisions about possible actions to take, and then feed the results back into the instrumentation and collection stages. The visualization/alerting stage phase presents a consumable view of the system state. It can display information in near real time by using a series of dashboards. And it can generate reports, graphs, and charts to provide a historical view of the data that can help identify long-term trends. If information indicates that a KPI is likely to exceed acceptable bounds, this stage can also trigger an alert to an operator. In some cases, an alert can also be used to trigger an automated process that attempts to take corrective actions, such as autoscaling.</p>
</div>
<div class="paragraph">
<p>Note that these steps constitute a continuous-flow process where the stages are happening in parallel. Ideally, all the phases should be dynamically configurable. At some points, especially when a system has been newly deployed or is experiencing problems, it might be necessary to gather extended data on a more frequent basis. At other times, it should be possible to revert to capturing a base level of essential information to verify that the system is functioning properly.</p>
</div>
<div class="paragraph">
<p>Additionally, the entire monitoring process should be considered a live, ongoing solution that&#8217;s subject to fine-tuning and improvements as a result of feedback. For example, you might start with measuring many factors to determine system health. Analysis over time might lead to a refinement as you discard measures that aren&#8217;t relevant, enabling you to more precisely focus on the data that you need while minimizing background noise.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sources_of_monitoring_and_diagnostic_data">Sources of monitoring and diagnostic data</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The information that the monitoring process uses can come from several sources, as illustrated in Figure 1. At the application level, information comes from trace logs incorporated into the code of the system. Developers should follow a standard approach for tracking the flow of control through their code. For example, an entry to a method can emit a trace message that specifies the name of the method, the current time, the value of each parameter, and any other pertinent information. Recording the entry and exit times can also prove useful.</p>
</div>
<div class="paragraph">
<p>You should log all exceptions and warnings, and ensure that you retain a full trace of any nested exceptions and warnings. Ideally, you should also capture information that identifies the user who is running the code, together with activity correlation information (to track requests as they pass through the system). And you should log attempts to access all resources such as message queues, databases, files, and other dependent services. This information can be used for metering and auditing purposes.</p>
</div>
<div class="paragraph">
<p>Many applications use libraries and frameworks to perform common tasks such as accessing a data store or communicating over a network. These frameworks might be configurable to provide their own trace messages and raw diagnostic information, such as transaction rates and data transmission successes and failures.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Many modern frameworks automatically publish performance and trace events. Capturing this information is simply a matter of providing a means to retrieve and store it where it can be processed and analyzed.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The operating system where the application is running can be a source of low-level system-wide information, such as performance counters that indicate I/O rates, memory utilization, and CPU usage. Operating system errors (such as the failure to open a file correctly) might also be reported.</p>
</div>
<div class="paragraph">
<p>You should also consider the underlying infrastructure and components on which your system runs. Virtual machines, virtual networks, and storage services can all be sources of important infrastructure-level performance counters and other diagnostic data.</p>
</div>
<div class="paragraph">
<p>If your application uses other external services, such as a web server or database management system, these services might publish their own trace information, logs, and performance counters. Examples include SQL Server Dynamic Management Views for tracking operations performed against a SQL Server database, and IIS trace logs for recording requests made to a web server.</p>
</div>
<div class="paragraph">
<p>As the components of a system are modified and new versions are deployed, it&#8217;s important to be able to attribute issues, events, and metrics to each version. This information should be tied back to the release pipeline so that problems with a specific version of a component can be tracked quickly and rectified.</p>
</div>
<div class="paragraph">
<p>Security issues might occur at any point in the system. For example, a user might attempt to sign in with an invalid user ID or password. An authenticated user might try to obtain unauthorized access to a resource. Or a user might provide an invalid or outdated key to access encrypted information. Security-related information for successful and failing requests should always be logged.</p>
</div>
<div class="paragraph">
<p>The section <a href="#">Instrumenting an application</a> contains more guidance on the information that you should capture. But you can use a variety of strategies to gather this information:</p>
</div>
<div class="dlist unordered">
<dl>
<dt>Application/system monitoring.</dt>
<dd>
<p>This strategy uses internal sources within the application, application frameworks, operating system, and infrastructure. The application code can generate its own monitoring data at notable points during the lifecycle of a client request. The application can include tracing statements that might be selectively enabled or disabled as circumstances dictate. It might also be possible to inject diagnostics dynamically by using a diagnostics framework. These frameworks typically provide plug-ins that can attach to various instrumentation points in your code and capture trace data at these points.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Additionally, your code and/or the underlying infrastructure might raise events at critical points. Monitoring agents that are configured to listen for these events can record the event information.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Real user monitoring.</dt>
<dd>
<p>This approach records the interactions between a user and the application and observes the flow of each request and response. This information can have a two-fold purpose: it can be used for metering usage by each user, and it can be used to determine whether users are receiving a suitable quality of service (for example, fast response times, low latency, and minimal errors). You can use the captured data to identify areas of concern where failures occur most often. You can also use the data to identify elements where the system slows down, possibly due to hotspots in the application or some other form of bottleneck. If you implement this approach carefully, it might be possible to reconstruct users' flows through the application for debugging and testing purposes.</p>
</dd>
</dl>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You should consider the data that&#8217;s captured by monitoring real users to be highly sensitive because it might include confidential material. If you save captured data, store it securely. If you want to use the data for performance monitoring or debugging purposes, strip out all personally identifiable information first.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Synthetic user monitoring.</dt>
<dd>
<p>In this approach, you write your own test client that simulates a user and performs a configurable but typical series of operations. You can track the performance of the test client to help determine the state of the system. You can also use multiple instances of the test client as part of a load-testing operation to establish how the system responds under stress, and what sort of monitoring output is generated under these conditions.</p>
</dd>
</dl>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You can implement real and synthetic user monitoring by including code that traces and times the execution of method calls and other critical parts of an application.
</td>
</tr>
</table>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Profiling.</dt>
<dd>
<p>This approach is primarily targeted at monitoring and improving application performance. Rather than operating at the functional level of real and synthetic user monitoring, it captures lower-level information as the application runs. You can implement profiling by using periodic sampling of the execution state of an application (determining which piece of code that the application is running at a given point in time). You can also use instrumentation that inserts probes into the code at important junctures (such as the start and end of a method call) and records which methods were invoked, at what time, and how long each call took. You can then analyze this data to determine which parts of the application might cause performance problems.</p>
</dd>
<dt class="hdlist1">Endpoint monitoring.</dt>
<dd>
<p>This technique uses one or more diagnostic endpoints that the application exposes specifically to enable monitoring. An endpoint provides a pathway into the application code and can return information about the health of the system. Different endpoints can focus on various aspects of the functionality. You can write your own diagnostics client that sends periodic requests to these endpoints and assimilate the responses. For more information, see the <a href="../../../../../design-patterns/health-endpoint-monitoring.html">Health Endpoint Monitoring pattern</a>.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>For maximum coverage, you should use a combination of these techniques.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_instrumenting_an_application">Instrumenting an application</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Instrumentation is a critical part of the monitoring process. You can make meaningful decisions about the performance and health of a system only if you first capture the data that enables you to make these decisions. The information that you gather by using instrumentation should be sufficient to enable you to assess performance, diagnose problems, and make decisions without requiring you to sign in to a remote production server to perform tracing (and debugging) manually. Instrumentation data typically comprises metrics and information that&#8217;s written to trace logs.</p>
</div>
<div class="paragraph">
<p>The <strong>contents of a trace log</strong> can be the result of textual data that&#8217;s written by the application or binary data that&#8217;s created as the result of a trace event (if the application is using Event Tracing for Windows&#8212;&#8203;ETW). They can also be generated from system logs that record events arising from parts of the infrastructure, such as a web server. Textual log messages are often designed to be human-readable, but they should also be written in a format that enables an automated system to parse them easily.</p>
</div>
<div class="paragraph">
<p>You should also categorize logs. Don&#8217;t write all trace data to a single log, but use separate logs to record the trace output from different operational aspects of the system. You can then quickly filter log messages by reading from the appropriate log rather than having to process a single lengthy file. <mark>Never write information that has different security requirements (such as audit information and debugging data) to the same log.</mark></p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
A log might be implemented as a file on the file system, or it might be held in some other format, such as a blob in blob storage. Log information might also be held in more structured storage, such as rows in a table.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><strong>Metrics</strong> will generally be a measure or count of some aspect or resource in the system at a specific time, with one or more associated tags or dimensions (sometimes called a <em>sample</em>). A single instance of a metric is usually not useful in isolation. Instead, metrics have to be captured over time. The key issue to consider is which metrics you should record and how frequently. Generating data for metrics too often can impose a significant additional load on the system, whereas capturing metrics infrequently might cause you to miss the circumstances that lead to a significant event. The considerations will vary from metric to metric. For example, CPU utilization on a server might vary significantly from second to second, but high utilization becomes an issue only if it&#8217;s long-lived over a number of minutes.</p>
</div>
<div class="sect2">
<h3 id="_information_for_correlating_data">Information for correlating data</h3>
<div class="paragraph">
<p>You can easily monitor individual system-level performance counters, capture metrics for resources, and obtain application trace information from various log files.But some forms of monitoring require the analysis and diagnostics stage in the monitoring pipeline to correlate the data that&#8217;s retrieved from several sources.This data might take several forms in the raw data, and the analysis process must be provided with sufficient instrumentation data to be able to map these different forms.For example, at the application framework level, a task might be identified by a thread ID.Within an application, the same work might be associated with the user ID for the user who is performing that task.</p>
</div>
<div class="paragraph">
<p>Also, there&#8217;s unlikely to be a 1:1 mapping between threads and user requests, because asynchronous operations might reuse the same threads to perform operations on behalf of more than one user.To complicate matters further, a single request might be handled by more than one thread as execution flows through the system.If possible, associate each request with a unique activity ID that&#8217;s propagated through the system as part of the request context.(The technique for generating and including activity IDs in trace information depends on the technology that&#8217;s used to capture the trace data.)</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>That unique activity ID is also known as <em>correlation ID</em> or <em>trace ID</em></p>
</li>
<li>
<p>All monitoring data should be timestamped in the same way.For consistency, record all dates and times by using Coordinated Universal Time.This will help you more easily trace sequences of events.</p>
</li>
<li>
<p>Computers operating in different time zones and networks might not be synchronized. Don&#8217;t depend on using timestamps alone for correlating instrumentation data that spans multiple machines.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_information_to_include_in_the_instrumentation_data">Information to include in the instrumentation data</h3>
<div class="paragraph">
<p>Consider the following points when you&#8217;re deciding which instrumentation data you need to collect:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Make sure that information captured by trace events is machine and human readable. Adopt well-defined schemas for this information to facilitate automated processing of log data across systems, and to provide consistency to operations and engineering staff reading the logs. <strong>Include environmental information, such as the deployment environment, the machine on which the process is running, the details of the process, and the call stack.</strong></p>
</li>
<li>
<p>Enable profiling only when necessary because it can impose a significant overhead on the system. <mark><strong>Profiling</strong> by using instrumentation records an event (such as a method call) every time it occurs, whereas <strong>sampling</strong> records only selected events.</mark> The selection can be time-based (once every n seconds), or frequency-based (once every n requests). If events occur very frequently, profiling by instrumentation might cause too much of a burden and itself affect overall performance. In this case, the sampling approach might be preferable. However, if the frequency of events is low, sampling might miss them. In this case, instrumentation might be the better approach.</p>
</li>
<li>
<p>Provide sufficient context to enable a developer or administrator to determine the source of each request. This might include some form of activity ID that identifies a specific instance of a request. It might also include information that can be used to correlate this activity with the computational work performed and the resources used. Note that this work might cross process and machine boundaries. For metering, the context should also include (either directly or indirectly via other correlated information) a reference to the customer who caused the request to be made. This context provides valuable information about the application state at the time that the monitoring data was captured.</p>
</li>
<li>
<p>Record all requests, and the locations or regions from which these requests are made. This information can assist in determining whether there are any location-specific hotspots. This information can also be useful in determining whether to repartition an application or the data that it uses.</p>
</li>
<li>
<p>Record and capture the details of exceptions carefully. Often, critical debug information is lost as a result of poor exception handling. Capture the full details of exceptions that the application throws, including any inner exceptions and other context information. Include the call stack if possible.</p>
</li>
<li>
<p>Be consistent in the data that the different elements of your application capture, because this can assist in analyzing events and correlating them with user requests. Consider using a comprehensive and configurable logging package to gather information, rather than depending on developers to adopt the same approach as they implement different parts of the system.</p>
<div class="ulist">
<ul>
<li>
<p>Gather data from key performance counters, such as</p>
<div class="ulist">
<ul>
<li>
<p>the volume of I/O being performed,</p>
</li>
<li>
<p>network utilization,</p>
</li>
<li>
<p>number of requests,</p>
</li>
<li>
<p>memory use,</p>
</li>
<li>
<p>and CPU utilization.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Some infrastructure services might provide their own specific performance counters, such as</p>
<div class="ulist">
<ul>
<li>
<p>the number of connections to a database,</p>
</li>
<li>
<p>the rate at which transactions are being performed,</p>
</li>
<li>
<p>and the number of transactions that succeed or fail.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Applications might also define their own specific performance counters.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Log all calls made to external services, such as database systems, web services, or other system-level services that are part of the infrastructure. Record information about the time taken to perform each call, and the success or failure of the call. If possible, capture information about all retry attempts and failures for any transient errors that occur.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_ensuring_compatibility_with_telemetry_systems">Ensuring compatibility with telemetry systems</h3>
<div class="paragraph">
<p>In many cases, the information that instrumentation produces is generated as a series of events and passed to a separate telemetry system for processing and analysis. A telemetry system is typically independent of any specific application or technology, but it expects information to follow a specific format that&#8217;s usually defined by a schema. The schema effectively specifies a contract that defines the data fields and types that the telemetry system can ingest. The schema should be generalized to allow for data arriving from a range of platforms and devices.</p>
</div>
<div class="paragraph">
<p>A common schema should include fields that are common to all instrumentation events, such as the event name, the event time, the IP address of the sender, and the details that are required for correlating with other events (such as a user ID, a device ID, and an application ID). Remember that any number of devices might raise events, so the schema should not depend on the device type. Additionally, various devices might raise events for the same application; the application might support roaming or some other form of cross-device distribution.</p>
</div>
<div class="paragraph">
<p>The schema might also include domain fields that are relevant to a particular scenario that&#8217;s common across different applications. This might be information about exceptions, application start and end events, and success and/or failure of web service API calls. All applications that use the same set of domain fields should emit the same set of events, enabling a set of common reports and analytics to be built.</p>
</div>
<div class="paragraph">
<p>Finally, a schema might contain custom fields for capturing the details of application-specific events.</p>
</div>
</div>
<div class="sect2">
<h3 id="_best_practices_for_instrumenting_applications">Best practices for instrumenting applications</h3>
<div class="paragraph">
<p>The following list summarizes best practices for instrumenting a distributed application running in the cloud.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Make logs easy to read and easy to parse. Use structured logging where possible. Be concise and descriptive in log messages.</p>
</li>
<li>
<p>In all logs, identify the source and provide context and timing information as each log record is written.</p>
</li>
<li>
<p>Use the same time zone and format for all timestamps. This will help to correlate events for operations that span hardware and services running in different geographic regions.</p>
</li>
<li>
<p>Categorize logs and write messages to the appropriate log file.</p>
</li>
<li>
<p>Do not disclose sensitive information about the system or personal information about users. Scrub this information before it&#8217;s logged, but ensure that the relevant details are retained. For example, remove the ID and password from any database connection strings, but write the remaining information to the log so that an analyst can determine that the system is accessing the correct database.</p>
<div class="ulist">
<ul>
<li>
<p>Log all critical exceptions, but enable the administrator to turn logging on and off for lower levels of exceptions and warnings.</p>
</li>
<li>
<p>Also, capture and log all retry logic information. This data can be useful in monitoring the transient health of the system.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Trace out of process calls, such as requests to external web services or databases.</p>
</li>
<li>
<p>Don&#8217;t mix log messages with different security requirements in the same log file. For example, don&#8217;t write debug and audit information to the same log.</p>
</li>
<li>
<p>With the exception of auditing events, make sure that all logging calls are fire-and-forget operations that do not block the progress of business operations. Auditing events are exceptional because they are critical to the business and can be classified as a fundamental part of business operations.</p>
</li>
<li>
<p>Make sure that logging is extensible and does not have any direct dependencies on a concrete target. For example, rather than writing information by using System.Diagnostics.Trace, define an abstract interface (such as ILogger) that exposes logging methods and that can be implemented through any appropriate means.</p>
</li>
<li>
<p>Make sure that all logging is fail-safe and never triggers any cascading errors. Logging must not throw any exceptions.</p>
</li>
<li>
<p>Treat instrumentation as an ongoing iterative process and review logs regularly, not just when there is a problem.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_collecting_and_storing_data">Collecting and storing data</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The collection stage of the monitoring process is concerned with retrieving the information that instrumentation generates, formatting this data to make it easier for the analysis/diagnosis stage to consume, and saving the transformed data in reliable storage. The instrumentation data that you gather from different parts of a distributed system can be held in a variety of locations and with varying formats. For example, your application code might generate trace log files and generate application event log data, whereas performance counters that monitor key aspects of the infrastructure that your application uses can be captured through other technologies. Any third-party components and services that your application uses might provide instrumentation information in different formats, by using separate trace files, blob storage, or even a custom data store.</p>
</div>
<div class="paragraph">
<p>Data collection is often performed through a collection service that can run autonomously from the application that generates the instrumentation data. Figure 2 illustrates an example of this architecture, highlighting the instrumentation data-collection subsystem.</p>
</div>
<div id="img-telemetryservice" class="imageblock">
<div class="content">
<img src="./images/telemetryservice.png" alt="telemetryservice">
</div>
<div class="title">Figure 2. Collecting instrumentation data.</div>
</div>
<div class="paragraph">
<p>Note that this is a simplified view. The collection service is not necessarily a single process and might comprise many constituent parts running on different machines, as described in the following sections. Additionally, if the analysis of some telemetry data must be performed quickly (hot analysis, as described in the section <a href="https://docs.microsoft.com/en-us/azure/architecture/best-practices/monitoring#supporting-hot-warm-and-cold-analysis">Supporting hot, warm, and cold analysis</a> later in this document), local components that operate outside the collection service might perform the analysis tasks immediately. Figure 2 depicts this situation for selected events. After analytical processing, the results can be sent directly to the visualization and alerting subsystem. Data that&#8217;s subjected to warm or cold analysis is held in storage while it awaits processing.</p>
</div>
<div class="paragraph">
<p>For Azure applications and services, Azure Diagnostics provides one possible solution for capturing data. Azure Diagnostics gathers data from the following sources for each compute node, aggregates it, and then uploads it to Azure Storage:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>IIS logs</p>
</li>
<li>
<p>IIS Failed Request logs</p>
</li>
<li>
<p>Windows event logs</p>
</li>
<li>
<p>Performance counters</p>
</li>
<li>
<p>Crash dumps</p>
</li>
<li>
<p>Azure Diagnostics infrastructure logs</p>
</li>
<li>
<p>Custom error logs</p>
</li>
<li>
<p>.NET EventSource</p>
</li>
<li>
<p>Manifest-based ETW</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For more information, see the article Azure: <a href="https://social.technet.microsoft.com/wiki/contents/articles/18146.windows-azure-telemetry-basics-and-troubleshooting.aspx">Telemetry Basics and Troubleshooting</a>.</p>
</div>
<div class="sect2">
<h3 id="_strategies_for_collecting_instrumentation_data">Strategies for collecting instrumentation data</h3>
<div class="paragraph">
<p>Considering the elastic nature of the cloud, and to avoid the necessity of manually retrieving telemetry data from every node in the system, you should arrange for the data to be transferred to a central location and consolidated. In a system that spans multiple datacenters, it might be useful to first collect, consolidate, and store data on a region-by-region basis, and then aggregate the regional data into a single central system.</p>
</div>
<div class="paragraph">
<p>To optimize the use of bandwidth, you can elect to transfer less urgent data in chunks, as batches. However, the data must not be delayed indefinitely, especially if it contains time-sensitive information.</p>
</div>
<div class="sect3">
<h4 id="_pulling_and_pushing_instrumentation_data">Pulling and pushing instrumentation data</h4>
<div class="paragraph">
<p>The instrumentation data-collection subsystem can actively retrieve instrumentation data from the various logs and other sources for each instance of the application (the <em>pull model</em>). Or, it can act as a passive receiver that waits for the data to be sent from the components that constitute each instance of the application (the<em> push model</em>).</p>
</div>
<div class="paragraph">
<p>One approach to implementing the pull model is to use monitoring agents that run locally with each instance of the application. A monitoring agent is a separate process that periodically retrieves (pulls) telemetry data collected at the local node and writes this information directly to centralized storage that all instances of the application share. This is the mechanism that Azure Diagnostics implements. Each instance of an Azure web or worker role can be configured to capture diagnostic and other trace information that&#8217;s stored locally. The monitoring agent that runs alongside each instance copies the specified data to Azure Storage. The article Enabling Diagnostics in Azure Cloud Services and Virtual Machines provides more details on this process. Some elements, such as IIS logs, crash dumps, and custom error logs, are written to blob storage. Data from the Windows event log, ETW events, and performance counters is recorded in table storage. Figure 3 illustrates this mechanism.</p>
</div>
<div id="image-pullmodel" class="imageblock">
<div class="content">
<img src="./images/pullmodel.png" alt="pullmodel">
</div>
<div class="title">Figure 3. Using a monitoring agent to pull information and write to shared storage.</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Using a monitoring agent is ideally suited to capturing instrumentation data that&#8217;s naturally pulled from a data source. An example is information from SQL Server Dynamic Management Views or the length of an Azure Service Bus queue.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>It&#8217;s feasible to use the approach just described to store telemetry data for a small-scale application running on a limited number of nodes in a single location. However, a complex, highly scalable, global cloud application might generate huge volumes of data from hundreds of web and worker roles, database shards, and other services. This flood of data can easily overwhelm the I/O bandwidth available with a single, central location. Therefore, your telemetry solution must be scalable to prevent it from acting as a bottleneck as the system expands. Ideally, your solution should incorporate a degree of redundancy to reduce the risks of losing important monitoring information (such as auditing or billing data) if part of the system fails.</p>
</div>
<div class="paragraph">
<p>To address these issues, you can implement queuing, as shown in Figure 4. In this architecture, the local monitoring agent (if it can be configured appropriately) or custom data-collection service (if not) posts data to a queue.A separate process running asynchronously (the storage writing service in Figure 4) takes the data in this queue and writes it to shared storage.A message queue is suitable for this scenario because it provides "at least once" semantics that help ensure that queued data will not be lost after it&#8217;s posted.You can implement the storage writing service by using a separate worker role.</p>
</div>
<div id="img-buffered-queue" class="imageblock">
<div class="content">
<img src="./images/bufferedqueue.png" alt="bufferedqueue">
</div>
<div class="title">Figure 4. Using a queue to buffer instrumentation data.</div>
</div>
<div class="paragraph">
<p>The local data-collection service can add data to a queue immediately after it&#8217;s received.The queue acts as a buffer, and the storage writing service can retrieve and write the data at its own pace.By default, a queue operates on a first-in, first-out basis.But you can prioritize messages to accelerate them through the queue if they contain data that must be handled more quickly.For more information, see the <a href="https://docs.microsoft.com/en-us/azure/architecture/patterns/priority-queue">Priority Queue pattern</a>.Alternatively, you can use different channels (such as Service Bus topics) to direct data to different destinations depending on the form of analytical processing that&#8217;s required.</p>
</div>
<div class="paragraph">
<p>For scalability, you can run multiple instances of the storage writing service.If there is a high volume of events, you can use an event hub to dispatch the data to different compute resources for processing and storage.</p>
</div>
</div>
<div class="sect3">
<h4 id="_consolidating_instrumentation_data">Consolidating instrumentation data</h4>
<div class="paragraph">
<p>The instrumentation data that the data-collection service retrieves from a single instance of an application gives a localized view of the health and performance of that instance.To assess the overall health of the system, it&#8217;s necessary to consolidate some aspects of the data in the local views.You can perform this after the data has been stored, but in some cases, you can also achieve it as the data is collected.Rather than being written directly to shared storage, the instrumentation data can pass through a separate data consolidation service that combines data and acts as a filter and cleanup process.For example, instrumentation data that includes the same correlation information such as an activity ID can be amalgamated.(It&#8217;s possible that a user starts performing a business operation on one node and then gets transferred to another node in the event of node failure, or depending on how load balancing is configured.) This process can also detect and remove any duplicated data (always a possibility if the telemetry service uses message queues to push instrumentation data out to storage).Figure 5 illustrates an example of this structure.</p>
</div>
<div id="img-consolidattion" class="imageblock">
<div class="content">
<img src="./images/consolidation.png" alt="consolidation">
</div>
<div class="title">Figure 5. Using a separate service to consolidate and clean up instrumentation data.</div>
</div>
</div>
<div class="sect3">
<h4 id="_storing_instrumentation_data">Storing instrumentation data</h4>
<div class="paragraph">
<p>The previous discussions have depicted a rather simplistic view of the way in which instrumentation data is stored.In reality, it can make sense to store the different types of information by using technologies that are most appropriate to the way in which each type is likely to be used.</p>
</div>
<div class="paragraph">
<p>For example, Azure blob and table storage have some similarities in the way in which they&#8217;re accessed.But they have limitations in the operations that you can perform by using them, and the granularity of the data that they hold is quite different.If you need to perform more analytical operations or require full-text search capabilities on the data, it might be more appropriate to use data storage that provides capabilities that are optimized for specific types of queries and data access.For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Performance counter data can be stored in a SQL database to enable ad hoc analysis.</p>
</li>
<li>
<p>Trace logs might be better stored in Azure Cosmos DB.</p>
</li>
<li>
<p>Security information can be written to HDFS.</p>
</li>
<li>
<p>Information that requires full-text search can be stored through Elasticsearch (which can also speed searches by using rich indexing).</p>
</li>
</ul>
</div>
<div id="img-datastorage" class="imageblock">
<div class="content">
<img src="./images/consolidation.png" alt="consolidation">
</div>
<div class="title">Figure 6. Partitioning data according to analytical and storage requirements.</div>
</div>
<div class="paragraph">
<p>The same instrumentation data might be required for more than one purpose. For example, performance counters can be used to provide a historical view of system performance over time. This information might be combined with other usage data to generate customer billing information. In these situations, the same data might be sent to more than one destination, such as a document database that can act as a long-term store for holding billing information, and a multidimensional store for handling complex performance analytics.</p>
</div>
<div class="paragraph">
<p>You should also consider how urgently the data is required. Data that provides information for alerting must be accessed quickly, so it should be held in fast data storage and indexed or structured to optimize the queries that the alerting system performs. In some cases, it might be necessary for the telemetry service that gathers the data on each node to format and save data locally so that a local instance of the alerting system can quickly notify you of any issues. The same data can be dispatched to the storage writing service shown in the previous diagrams and stored centrally if it&#8217;s also required for other purposes.</p>
</div>
<div class="paragraph">
<p>Information that&#8217;s used for more considered analysis, for reporting, and for spotting historical trends is less urgent and can be stored in a manner that supports data mining and ad hoc queries. For more information, see the section <a href="https://docs.microsoft.com/en-us/azure/architecture/best-practices/monitoring#supporting-hot-warm-and-cold-analysis">Supporting hot, warm, and cold analysis</a> later in this document.</p>
</div>
</div>
<div class="sect3">
<h4 id="_log_rotation_and_data_retention">Log rotation and data retention</h4>
<div class="paragraph">
<p>Instrumentation can generate considerable volumes of data. This data can be held in several places, starting with the raw log files, trace files, and other information captured at each node to the consolidated, cleaned, and partitioned view of this data held in shared storage. In some cases, after the data has been processed and transferred, the original raw source data can be removed from each node. In other cases, it might be necessary or simply useful to save the raw information. For example, data that&#8217;s generated for debugging purposes might be best left available in its raw form but can then be discarded quickly after any bugs have been rectified.</p>
</div>
<div class="paragraph">
<p>Performance data often has a longer life so that it can be used for spotting performance trends and for capacity planning. The consolidated view of this data is usually kept online for a finite period to enable fast access. After that, it can be archived or discarded.</p>
</div>
<div class="paragraph">
<p>Data gathered for metering and billing customers might need to be saved indefinitely.</p>
</div>
<div class="paragraph">
<p>Additionally, regulatory requirements might dictate that information collected for auditing and security purposes also needs to be archived and saved. This data is also sensitive and might need to be encrypted or otherwise protected to prevent tampering. You should never record users' passwords or other information that might be used to commit identity fraud. Such details should be scrubbed from the data before it&#8217;s stored.</p>
</div>
</div>
<div class="sect3">
<h4 id="_down_sampling">Down-sampling</h4>
<div class="paragraph">
<p>It&#8217;s useful to store historical data so that you can spot long-term trends. Rather than saving old data in its entirety, it might be possible to down-sample the data to reduce its resolution and save storage costs. As an example, rather than saving minute-by-minute performance indicators, you can consolidate data that&#8217;s more than a month old to form an hour-by-hour view.</p>
</div>
</div>
<div class="sect3">
<h4 id="_best_practices_for_collecting_and_storing_logging_information">Best practices for collecting and storing logging information</h4>
<div class="paragraph">
<p>The following list summarizes best practices for capturing and storing logging information:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The monitoring agent or data-collection service should run as an out-of-process service and should be simple to deploy.</p>
</li>
<li>
<p>All output from the monitoring agent or data-collection service should be an agnostic format that&#8217;s independent of the machine, operating system, or network protocol. For example, emit information in a self-describing format such as JSON, MessagePack, or Protobuf rather than ETL/ETW. Using a standard format enables the system to construct processing pipelines; components that read, transform, and send data in the agreed format can be easily integrated.</p>
</li>
<li>
<p>The monitoring and data-collection process must be fail-safe and must not trigger any cascading error conditions.</p>
</li>
<li>
<p>In the event of a transient failure in sending information to a data sink, the monitoring agent or data-collection service should be prepared to reorder telemetry data so that the newest information is sent first. (The monitoring agent/data-collection service might elect to drop the older data, or save it locally and transmit it later to catch up, at its own discretion.)</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_analyzing_data_and_diagnosing_issues">Analyzing data and diagnosing issues</h2>
<div class="sectionbody">
<div class="paragraph">
<p>An important part of the monitoring and diagnostics process is analyzing the gathered data to obtain a picture of the overall well-being of the system. You should have defined your own KPIs and performance metrics, and it&#8217;s important to understand how you can structure the data that has been gathered to meet your analysis requirements. It&#8217;s also important to understand how the data that&#8217;s captured in different metrics and log files is correlated, because this information can be key to tracking a sequence of events and help diagnose problems that arise.</p>
</div>
<div class="paragraph">
<p>As described in the section <a href="#_consolidating_instrumentation_data">Consolidating instrumentation data</a>, the data for each part of the system is typically captured locally, but it generally needs to be combined with data generated at other sites that participate in the system. This information requires careful correlation to ensure that data is combined accurately. For example, the usage data for an operation might span a node that hosts a website to which a user connects, a node that runs a separate service accessed as part of this operation, and data storage held on another node. This information needs to be tied together to provide an overall view of the resource and processing usage for the operation. Some preprocessing and filtering of data might occur on the node on which the data is captured, whereas aggregation and formatting are more likely to occur on a central node.</p>
</div>
<div class="sect2">
<h3 id="_supporting_hot_warm_and_cold_analysis">Supporting hot, warm, and cold analysis</h3>
<div class="sect3">
<h4 id="_hot_analysis">hot analysis</h4>
<div class="paragraph">
<p>Analyzing and reformatting data for visualization, reporting, and alerting purposes can be a complex process that consumes its own set of resources. Some forms of monitoring are time-critical and require immediate analysis of data to be effective. This is known as <em>hot analysis</em>. Examples include the analysis that are required for alerting and some aspects of security monitoring (such as detecting an attack on the system). Data that&#8217;s required for these purposes must be quickly available and structured for efficient processing. In some cases, it might be necessary to move the analysis processing to the individual nodes where the data is held.</p>
</div>
</div>
<div class="sect3">
<h4 id="_warm_analysis">warm analysis</h4>
<div class="paragraph">
<p>Other forms of analysis are less time-critical and might require some computation and aggregation after the raw data has been received.This is called <em>warm analysis</em>.Performance analysis often falls into this category.In this case, an isolated, single performance event is unlikely to be statistically significant.(It might be caused by a sudden spike or glitch.) The data from a series of events should provide a more reliable picture of system performance.</p>
</div>
<div class="paragraph">
<p>Warm analysis can also be used to help diagnose health issues.A health event is typically processed through hot analysis and can raise an alert immediately.An operator should be able to drill into the reasons for the health event by examining the data from the warm path.This data should contain information about the events leading up to the issue that caused the health event.</p>
</div>
</div>
<div class="sect3">
<h4 id="_cold_analysis">cold analysis</h4>
<div class="paragraph">
<p>Some types of monitoring generate more long-term data.This analysis can be performed at a later date, possibly according to a predefined schedule.In some cases, the analysis might need to perform complex filtering of large volumes of data captured over a period of time.This is called <em>cold analysis</em>.The key requirement is that the data is stored safely after it has been captured.For example, usage monitoring and auditing require an accurate picture of the state of the system at regular points in time, but this state information does not have to be available for processing immediately after it has been gathered.</p>
</div>
<div class="paragraph">
<p>An operator can also use cold analysis to provide the data for predictive health analysis.The operator can gather historical information over a specified period and use it in conjunction with the current health data (retrieved from the hot path) to spot trends that might soon cause health issues.In these cases, it might be necessary to raise an alert so that corrective action can be taken.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_correlating_data">Correlating data</h3>
<div class="paragraph">
<p>The data that instrumentation captures can provide a snapshot of the system state, but the purpose of analysis is to make this data actionable.For example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>What has caused an intense I/O loading at the system level at a specific time?</p>
</li>
<li>
<p>Is it the result of a large number of database operations?</p>
</li>
<li>
<p>Is this reflected in the database response times, the number of transactions per second, and application response times at the same juncture?</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If so, one remedial action that might reduce the load might be to shard the data over more servers.In addition, exceptions can arise as a result of a fault in any level of the system.An exception in one level often triggers another fault in the level above.</p>
</div>
<div class="paragraph">
<p>For these reasons, you need to be able to correlate the different types of monitoring data at each level to produce an overall view of the state of the system and the applications that are running on it.You can then use this information to make decisions about whether the system is functioning acceptably or not, and determine what can be done to improve the quality of the system.</p>
</div>
<div class="paragraph">
<p>As described in the section <a href="#_information_for_correlating_data">Information for correlating data</a>, you must ensure that the raw instrumentation data includes sufficient context and activity ID information to support the required aggregations for correlating events. Additionally, this data might be held in different formats, and it might be necessary to parse this information to convert it into a standardized format for analysis.</p>
</div>
</div>
<div class="sect2">
<h3 id="_troubleshooting_and_diagnosing_issues">Troubleshooting and diagnosing issues</h3>
<div class="paragraph">
<p>Diagnosis requires the ability to determine the cause of faults or unexpected behavior, including performing root cause analysis. The information that&#8217;s required typically includes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Detailed information from event logs and traces, either for the entire system or for a specified subsystem during a specified time window.</p>
</li>
<li>
<p>Complete stack traces resulting from exceptions and faults of any specified level that occur within the system or a specified subsystem during a specified period.</p>
</li>
<li>
<p>Crash dumps for any failed processes either anywhere in the system or for a specified subsystem during a specified time window.</p>
</li>
<li>
<p>Activity logs recording the operations that are performed either by all users or for selected users during a specified period.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Analyzing data for troubleshooting purposes often requires a deep technical understanding of the system architecture and the various components that compose the solution. As a result, a large degree of manual intervention is often required to interpret the data, establish the cause of problems, and recommend an appropriate strategy to correct them. It might be appropriate simply to store a copy of this information in its original format and make it available for cold analysis by an expert.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_visualization_by_using_dashboards">Visualization by using dashboards</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The most common way to visualize data is to use dashboards that can display information as a series of charts, graphs, or some other illustration. These items can be parameterized, and an analyst should be able to select the important parameters (such as the time period) for any specific situation.</p>
</div>
<div class="paragraph">
<p>Dashboards can be organized hierarchically. Top-level dashboards can give an overall view of each aspect of the system but enable an operator to drill down to the details. For example, a dashboard that depicts the overall disk I/O for the system should allow an analyst to view the I/O rates for each individual disk to ascertain whether one or more specific devices account for a disproportionate volume of traffic. Ideally, the dashboard should also display related information, such as the source of each request (the user or activity) that&#8217;s generating this I/O. This information can then be used to determine whether (and how) to spread the load more evenly across devices, and whether the system would perform better if more devices were added.</p>
</div>
<div class="paragraph">
<p>A dashboard might also use color-coding or some other visual cues to indicate values that appear anomalous or that are outside an expected range. Using the previous example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A disk with an I/O rate that&#8217;s approaching its maximum capacity over an extended period (a hot disk) can be highlighted in red.</p>
</li>
<li>
<p>A disk with an I/O rate that periodically runs at its maximum limit over short periods (a warm disk) can be highlighted in yellow.</p>
</li>
<li>
<p>A disk that&#8217;s exhibiting normal usage can be displayed in green.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Note that for a dashboard system to work effectively, it must have the raw data to work with. If you are building your own dashboard system, or using a dashboard developed by another organization, you must understand which instrumentation data you need to collect, at what levels of granularity, and how it should be formatted for the dashboard to consume.</p>
</div>
<div class="paragraph">
<p>A good dashboard does not only display information, it also enables an analyst to pose ad hoc questions about that information. Some systems provide management tools that an operator can use to perform these tasks and explore the underlying data. Alternatively, depending on the repository that&#8217;s used to hold this information, it might be possible to query this data directly, or import it into tools such as Microsoft Excel for further analysis and reporting.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You should restrict access to dashboards to authorized personnel, because this information might be commercially sensitive. You should also protect the underlying data for dashboards to prevent users from changing it.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_raising_alerts">Raising alerts</h3>
<div class="paragraph">
<p>Alerting is the process of analyzing the monitoring and instrumentation data and generating a notification if a significant event is detected.</p>
</div>
<div class="paragraph">
<p>Alerting helps ensure that the system remains healthy, responsive, and secure. It&#8217;s an important part of any system that makes performance, availability, and privacy guarantees to the users where the data might need to be acted on immediately. An operator might need to be notified of the event that triggered the alert. Alerting can also be used to invoke system functions such as autoscaling.</p>
</div>
<div class="paragraph">
<p>Alerting usually depends on the following instrumentation data:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Security events.</dt>
<dd>
<p>If the event logs indicate that repeated authentication and/or authorization failures are occurring, the system might be under attack and an operator should be informed.</p>
</dd>
<dt class="hdlist1">Performance metrics.</dt>
<dd>
<p>The system must quickly respond if a particular performance metric exceeds a specified threshold.</p>
</dd>
<dt class="hdlist1">Availability information.</dt>
<dd>
<p>If a fault is detected, it might be necessary to quickly restart one or more subsystems, or fail over to a backup resource. Repeated faults in a subsystem might indicate more serious concerns.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Operators might receive alert information by using many delivery channels such as email, a pager device, or an SMS text message. An alert might also include an indication of how critical a situation is. Many alerting systems support subscriber groups, and all operators who are members of the same group can receive the same set of alerts.</p>
</div>
<div class="paragraph">
<p>An alerting system should be customizable, and the appropriate values from the underlying instrumentation data can be provided as parameters. This approach enables an operator to filter data and focus on those thresholds or combinations of values that are of interest. Note that in some cases, the raw instrumentation data can be provided to the alerting system. In other situations, it might be more appropriate to supply aggregated data. (For example, an alert can be triggered if the CPU utilization for a node has exceeded 90 percent over the last 10 minutes). The details provided to the alerting system should also include any appropriate summary and context information. This data can help reduce the possibility that false-positive events will trip an alert.</p>
</div>
</div>
<div class="sect2">
<h3 id="_reporting">Reporting</h3>
<div class="paragraph">
<p>Reporting is used to generate an overall view of the system. It might incorporate historical data in addition to current information. Reporting requirements themselves fall into two broad categories: operational reporting and security reporting.</p>
</div>
<div class="paragraph">
<p>Operational reporting typically includes the following aspects:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Aggregating statistics that you can use to understand resource utilization of the overall system or specified subsystems during a specified time window.</p>
</li>
<li>
<p>Identifying trends in resource usage for the overall system or specified subsystems during a specified period.</p>
</li>
<li>
<p>Monitoring the exceptions that have occurred throughout the system or in specified subsystems during a specified period.</p>
</li>
<li>
<p>Determining the efficiency of the application in terms of the deployed resources, and understanding whether the volume of resources (and their associated cost) can be reduced without affecting performance unnecessarily.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Security reporting is concerned with tracking customers' use of the system. It can include:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Auditing user operations.</dt>
<dd>
<p>This requires recording the individual requests that each user performs, together with dates and times. The data should be structured to enable an administrator to quickly reconstruct the sequence of operations that a user performs over a specified period.</p>
</dd>
<dt class="hdlist1">Tracking resource use by user.</dt>
<dd>
<p>This requires recording how each request for a user accesses the various resources that compose the system, and for how long. An administrator must be able to use this data to generate a utilization report by user over a specified period, possibly for billing purposes.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>In many cases, batch processes can generate reports according to a defined schedule. (Latency is not normally an issue.) But they should also be available for generation on an ad hoc basis if needed. As an example, if you are storing data in a relational database such as Azure SQL Database, you can use a tool such as SQL Server Reporting Services to extract and format data and present it as a set of reports.</p>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-06-22 13:10:08 UTC
</div>
</div>
</body>
</html>