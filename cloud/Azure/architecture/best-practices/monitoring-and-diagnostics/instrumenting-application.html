<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<title>Instrumenting an application</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/darshandsoni/asciidoctor-skins/css/material-teal.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>Instrumenting an application</h1>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_information_for_correlating_data">Information for correlating data</a></li>
<li><a href="#_information_to_include_in_the_instrumentation_data">Information to include in the instrumentation data</a></li>
<li><a href="#_ensuring_compatibility_with_telemetry_systems">Ensuring compatibility with telemetry systems</a></li>
<li><a href="#_best_practices_for_instrumenting_applications">Best practices for instrumenting applications</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Instrumentation is a critical part of the monitoring process. You can make meaningful decisions about the performance and health of a system only if you first capture the data that enables you to make these decisions. The information that you gather by using instrumentation should be sufficient to enable you to assess performance, diagnose problems, and make decisions without requiring you to sign in to a remote production server to perform tracing (and debugging) manually. Instrumentation data typically comprises metrics and information that&#8217;s written to trace logs.</p>
</div>
<div class="paragraph">
<p>The <strong>contents of a trace log</strong> can be the result of textual data that&#8217;s written by the application or binary data that&#8217;s created as the result of a trace event (if the application is using Event Tracing for Windows&#8212;&#8203;ETW). They can also be generated from system logs that record events arising from parts of the infrastructure, such as a web server. Textual log messages are often designed to be human-readable, but they should also be written in a format that enables an automated system to parse them easily.</p>
</div>
<div class="paragraph">
<p>You should also categorize logs. Don&#8217;t write all trace data to a single log, but use separate logs to record the trace output from different operational aspects of the system. You can then quickly filter log messages by reading from the appropriate log rather than having to process a single lengthy file. <mark>Never write information that has different security requirements (such as audit information and debugging data) to the same log.</mark></p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
A log might be implemented as a file on the file system, or it might be held in some other format, such as a blob in blob storage. Log information might also be held in more structured storage, such as rows in a table.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><strong>Metrics</strong> will generally be a measure or count of some aspect or resource in the system at a specific time, with one or more associated tags or dimensions (sometimes called a <em>sample</em>). A single instance of a metric is usually not useful in isolation. Instead, metrics have to be captured over time. The key issue to consider is which metrics you should record and how frequently. Generating data for metrics too often can impose a significant additional load on the system, whereas capturing metrics infrequently might cause you to miss the circumstances that lead to a significant event. The considerations will vary from metric to metric. For example, CPU utilization on a server might vary significantly from second to second, but high utilization becomes an issue only if it&#8217;s long-lived over a number of minutes.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_information_for_correlating_data">Information for correlating data</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can easily monitor individual system-level performance counters, capture metrics for resources, and obtain application trace information from various log files.But some forms of monitoring require the analysis and diagnostics stage in the monitoring pipeline to correlate the data that&#8217;s retrieved from several sources.This data might take several forms in the raw data, and the analysis process must be provided with sufficient instrumentation data to be able to map these different forms.For example, at the application framework level, a task might be identified by a thread ID.Within an application, the same work might be associated with the user ID for the user who is performing that task.</p>
</div>
<div class="paragraph">
<p>Also, there&#8217;s unlikely to be a 1:1 mapping between threads and user requests, because asynchronous operations might reuse the same threads to perform operations on behalf of more than one user.To complicate matters further, a single request might be handled by more than one thread as execution flows through the system.If possible, associate each request with a unique activity ID that&#8217;s propagated through the system as part of the request context.(The technique for generating and including activity IDs in trace information depends on the technology that&#8217;s used to capture the trace data.)</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>That unique activity ID is also known as <em>correlation ID</em> or <em>trace ID</em></p>
</li>
<li>
<p>All monitoring data should be timestamped in the same way.For consistency, record all dates and times by using Coordinated Universal Time.This will help you more easily trace sequences of events.</p>
</li>
<li>
<p>Computers operating in different time zones and networks might not be synchronized. Don&#8217;t depend on using timestamps alone for correlating instrumentation data that spans multiple machines.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_information_to_include_in_the_instrumentation_data">Information to include in the instrumentation data</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Consider the following points when you&#8217;re deciding which instrumentation data you need to collect:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Make sure that information captured by trace events is machine and human readable. Adopt well-defined schemas for this information to facilitate automated processing of log data across systems, and to provide consistency to operations and engineering staff reading the logs. <strong>Include environmental information, such as the deployment environment, the machine on which the process is running, the details of the process, and the call stack.</strong></p>
</li>
<li>
<p>Enable profiling only when necessary because it can impose a significant overhead on the system. <mark><strong>Profiling</strong> by using instrumentation records an event (such as a method call) every time it occurs, whereas <strong>sampling</strong> records only selected events.</mark> The selection can be time-based (once every n seconds), or frequency-based (once every n requests). If events occur very frequently, profiling by instrumentation might cause too much of a burden and itself affect overall performance. In this case, the sampling approach might be preferable. However, if the frequency of events is low, sampling might miss them. In this case, instrumentation might be the better approach.</p>
</li>
<li>
<p>Provide sufficient context to enable a developer or administrator to determine the source of each request. This might include some form of activity ID that identifies a specific instance of a request. It might also include information that can be used to correlate this activity with the computational work performed and the resources used. Note that this work might cross process and machine boundaries. For metering, the context should also include (either directly or indirectly via other correlated information) a reference to the customer who caused the request to be made. This context provides valuable information about the application state at the time that the monitoring data was captured.</p>
</li>
<li>
<p>Record all requests, and the locations or regions from which these requests are made. This information can assist in determining whether there are any location-specific hotspots. This information can also be useful in determining whether to repartition an application or the data that it uses.</p>
</li>
<li>
<p>Record and capture the details of exceptions carefully. Often, critical debug information is lost as a result of poor exception handling. Capture the full details of exceptions that the application throws, including any inner exceptions and other context information. Include the call stack if possible.</p>
</li>
<li>
<p>Be consistent in the data that the different elements of your application capture, because this can assist in analyzing events and correlating them with user requests. Consider using a comprehensive and configurable logging package to gather information, rather than depending on developers to adopt the same approach as they implement different parts of the system.</p>
<div class="ulist">
<ul>
<li>
<p>Gather data from key performance counters, such as</p>
<div class="ulist">
<ul>
<li>
<p>the volume of I/O being performed,</p>
</li>
<li>
<p>network utilization,</p>
</li>
<li>
<p>number of requests,</p>
</li>
<li>
<p>memory use,</p>
</li>
<li>
<p>and CPU utilization.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Some infrastructure services might provide their own specific performance counters, such as</p>
<div class="ulist">
<ul>
<li>
<p>the number of connections to a database,</p>
</li>
<li>
<p>the rate at which transactions are being performed,</p>
</li>
<li>
<p>and the number of transactions that succeed or fail.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Applications might also define their own specific performance counters.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Log all calls made to external services, such as database systems, web services, or other system-level services that are part of the infrastructure. Record information about the time taken to perform each call, and the success or failure of the call. If possible, capture information about all retry attempts and failures for any transient errors that occur.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_ensuring_compatibility_with_telemetry_systems">Ensuring compatibility with telemetry systems</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In many cases, the information that instrumentation produces is generated as a series of events and passed to a separate telemetry system for processing and analysis. A telemetry system is typically independent of any specific application or technology, but it expects information to follow a specific format that&#8217;s usually defined by a schema. The schema effectively specifies a contract that defines the data fields and types that the telemetry system can ingest. The schema should be generalized to allow for data arriving from a range of platforms and devices.</p>
</div>
<div class="paragraph">
<p>A common schema should include fields that are common to all instrumentation events, such as the event name, the event time, the IP address of the sender, and the details that are required for correlating with other events (such as a user ID, a device ID, and an application ID). Remember that any number of devices might raise events, so the schema should not depend on the device type. Additionally, various devices might raise events for the same application; the application might support roaming or some other form of cross-device distribution.</p>
</div>
<div class="paragraph">
<p>The schema might also include domain fields that are relevant to a particular scenario that&#8217;s common across different applications. This might be information about exceptions, application start and end events, and success and/or failure of web service API calls. All applications that use the same set of domain fields should emit the same set of events, enabling a set of common reports and analytics to be built.</p>
</div>
<div class="paragraph">
<p>Finally, a schema might contain custom fields for capturing the details of application-specific events.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_best_practices_for_instrumenting_applications">Best practices for instrumenting applications</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following list summarizes best practices for instrumenting a distributed application running in the cloud.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Make logs easy to read and easy to parse. Use structured logging where possible. Be concise and descriptive in log messages.</p>
</li>
<li>
<p>In all logs, identify the source and provide context and timing information as each log record is written.</p>
</li>
<li>
<p>Use the same time zone and format for all timestamps. This will help to correlate events for operations that span hardware and services running in different geographic regions.</p>
</li>
<li>
<p>Categorize logs and write messages to the appropriate log file.</p>
</li>
<li>
<p>Do not disclose sensitive information about the system or personal information about users. Scrub this information before it&#8217;s logged, but ensure that the relevant details are retained. For example, remove the ID and password from any database connection strings, but write the remaining information to the log so that an analyst can determine that the system is accessing the correct database.</p>
<div class="ulist">
<ul>
<li>
<p>Log all critical exceptions, but enable the administrator to turn logging on and off for lower levels of exceptions and warnings.</p>
</li>
<li>
<p>Also, capture and log all retry logic information. This data can be useful in monitoring the transient health of the system.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Trace out of process calls, such as requests to external web services or databases.</p>
</li>
<li>
<p>Don&#8217;t mix log messages with different security requirements in the same log file. For example, don&#8217;t write debug and audit information to the same log.</p>
</li>
<li>
<p>With the exception of auditing events, make sure that all logging calls are fire-and-forget operations that do not block the progress of business operations. Auditing events are exceptional because they are critical to the business and can be classified as a fundamental part of business operations.</p>
</li>
<li>
<p>Make sure that logging is extensible and does not have any direct dependencies on a concrete target. For example, rather than writing information by using System.Diagnostics.Trace, define an abstract interface (such as ILogger) that exposes logging methods and that can be implemented through any appropriate means.</p>
</li>
<li>
<p>Make sure that all logging is fail-safe and never triggers any cascading errors. Logging must not throw any exceptions.</p>
</li>
<li>
<p>Treat instrumentation as an ongoing iterative process and review logs regularly, not just when there is a problem.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-06-22 13:10:08 UTC
</div>
</div>
</body>
</html>