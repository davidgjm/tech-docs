= Distributed Systems Observability
:toc:
:icons: font
:source-highlighter: rouge
:imagesdir: ./images

.References
[sidebar]
****
- https://www.pdl.cmu.edu/PDL-FTP/SelfStar/CMU-PDL-14-102.pdf[CMU Paper - key design insights for distributed system tracing]
- https://www.baeldung.com/distributed-systems-observability[baeldung - Observability in Distributed Systems]
- https://blog.twitter.com/engineering/en_us/a/2013/observability-at-twitter[Observability at Twitter]
- https://www.infoq.com/news/2020/09/observability-distributed-system/[Observability Strategies for Distributed Systems - Lessons Learned at InfoQ Live]
- https://blog.netsil.com/beyond-google-sre-what-is-site-reliability-engineering-like-at-medium-71c65bd35f4e[Beyond Google SRE: What is Site Reliability Engineering like at Medium?]
- https://github.com/keyvanakbary/learning-notes/blob/master/books/distributed-systems-observability.md
- https://docs.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-overview[OpenTelemetry overview]
- https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html[Chapter 4. The Three Pillars of Observability, Distributed Systems Observability] by Cindy Sridharan.
- https://opentelemetry.io/docs/concepts/data-sources/
- https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/overview.md[OpenTelemetry Specification]

- https://en.wikipedia.org/wiki/Reliable_Event_Logging_Protocol[Reliable Event Logging Protocol]


****

Logs, metrics, and traces are often known as the three pillars of observability. While plainly having access to logs, metrics, and traces doesn’t necessarily make systems more observable, these are powerful tools that, if understood well, can unlock the ability to build better systems.

== What Is Observability?
Let's cut to the chase and get the formal definition out to begin with! Observability is *the ability to measure the internal state of a system only by its external outputs*.

For a distributed system like microservices, these external outputs are basically known as telemetry data. It includes information like the resource consumption of a machine, the logs generated by the applications running on a machine, and several others.

=== Types of Telemetry Data
Telemetry, the data collected to observe your application, can be broken into three types or "pillars":

. Distributed Tracing
. Metrics
. Logs

*Logs are lines of text that an application generates at discrete points* during the execution of the code. Normally these are structured and often generated at different levels of severity. These are quite easy to generate but often carry performance costs. Moreover, we may require additional tools like Logstash to collect, store, and analyze logs efficiently.

Simply put, *metrics are values represented as counts or measures that we calculate or aggregate* over a time period. These values express some data about a system like a virtual machine — for instance, the memory consumption of a virtual machine every second. These can come from various sources like the host, application, and cloud platform.

Traces are important for distributed systems where a single request can flow through multiple applications. *A trace is a representation of distributed events as the request flows through a distributed system.* These can be quite helpful in locating the problems like bottlenecks, defects, or other issues in a distributed system.


== Logs

https://www.oreilly.com/library/view/distributed-systems-observability/9781492033431/ch04.html[Event Logs]

An _event log_ is an immutable, timestamped record of discrete events that happened over time. Event logs in general come in three forms but are fundamentally the same: a timestamp and a payload of some context. The three forms are:

Plaintext:: A log record might be free-form text. This is also the most common format of logs.
Structured:: Much evangelized and advocated for in recent days. Typically, these logs are emitted in the JSON format.
Binary:: Think logs in the Protobuf format, MySQL binlogs used for replication and point-in-time recovery, systemd journal logs, the `pflog` format used by the BSD firewall `pf` that often serves as a frontend to `tcpdump`.

Failures in complex distributed systems rarely arise because of one specific event happening in one specific component of the system. Often, various possible triggers across a highly interconnected graph of components are involved. By simply looking at discrete events that occurred in any given system at some point in time, it becomes impossible to determine all such triggers. To nail down the different triggers, one needs to be able to do the following:

- Start with a symptom pinpointed by a high-level metric or a log event in a specific system

- Infer the request lifecycle across different components of the distributed architecture

- Iteratively ask questions about interactions among various parts of the system

In addition to inferring the fate of a request throughout its lifecycle (which is usually short lived), it also becomes necessary to be able to infer the fate of a system as a whole (measured over a duration that is orders of magnitudes longer than the lifecycle of a single request).

Traces and metrics are an abstraction built on top of logs that pre-process and encode information along two orthogonal axes, one being request-centric (trace), the other being system-centric (metric).

=== The Pros and Cons of Logs
Logs perform really well in terms of surfacing highly granular information pregnant with rich local context, so long as the search space is localized to events that occurred in a single service.

The utility of logs, unfortunately, ends right there. While log _generation_ might be easy, the performance idiosyncrasies of various popular logging libraries leave a lot to be desired. Most performant logging libraries allocate very little, if any, and are extremely fast. However, the default logging libraries of many languages and frameworks are not the cream of the crop, which means the application as a whole becomes susceptible to suboptimal performance due to the overhead of logging. Additionally, log messages can also be lost unless one uses a protocol like https://en.wikipedia.org/wiki/Reliable_Event_Logging_Protocol[RELP] to guarantee reliable delivery of messages. This becomes especially important when log data is used for billing or payment purposes.

Last, unless the logging library can dynamically sample logs, logging excessively has the capability to adversely affect application performance as a whole. This is exacerbated when the logging isn’t asynchronous and request processing is blocked while writing a log line to disk or `stdout`.

On the processing side, raw logs are almost always normalized, filtered, and processed by a tool like Logstash, fluentd, Scribe, or Heka before they’re persisted in a data store like Elasticsearch or BigQuery. If an application generates a large volume of logs, then the logs might require further buffering in a broker like Kafka before they can be processed by Logstash. Hosted solutions like BigQuery have quotas one cannot exceed.

On the storage side, while Elasticsearch might be a fantastic search engine, running it carries a real operational cost. Even if an organization is staffed with a team of operations engineers who are experts in operating Elasticsearch, other drawbacks may exist. Case in point: it’s not uncommon to see a sharp downward slope in the graphs in Kibana, not because traffic to the service is dropping, but because Elasticsearch cannot keep up with the indexing of the sheer volume of data being thrown at it. Even if log ingestion processing isn’t an issue with Elasticsearch, no one I know of seems to have fully figured out how to use Kibana’s UI, let alone enjoy using it.

== Metrics

Metrics are a numeric representation of data measured over intervals of time. Metrics can harness the power of mathematical modeling and prediction to derive knowledge of the behavior of a system over intervals of time in the present and future.

Since numbers are optimized for storage, processing, compression, and retrieval, metrics enable longer retention of data as well as easier querying. This makes metrics perfectly suited to building dashboards that reflect historical trends. Metrics also allow for gradual reduction of data resolution. After a certain period of time, data can be aggregated into daily or weekly frequency.

=== The Drawbacks of Metrics
The biggest drawback with both application logs and application metrics is that they are _system_ scoped, making it hard to understand anything else other than what’s happening inside a particular system. Sure, metrics can also be request scoped, but that entails a concomitant increase in label fan-out, which results in an increase in metric storage.

With logs without fancy joins, a single line doesn’t give much information about what happened to a request across all components of a system. While it’s possible to construct a system that correlates metrics and logs across the address space or RPC boundaries, such systems require a metric to carry a UID as a label.

Using high cardinality values like UIDs as metric labels can overwhelm time-series databases. Although the new Prometheus storage engine has been optimized to handle time-series churn, longer time-range queries will still be slow. Prometheus was just an example. All popular existing time-series database solutions suffer performance under high cardinality labeling.

When used optimally, logs and metrics give us complete omniscience into a silo, but nothing more. While these might be sufficient for understanding the performance and behavior of individual systems, both stateful and stateless, they aren’t sufficient to understand the lifetime of a request that traverses multiple systems.

Distributed tracing is a technique that addresses the problem of bringing visibility into the lifetime of a request across several systems.

== Tracing
A _trace_ is a representation of a series of causally related distributed events that encode the end-to-end request flow through a distributed system.

Traces are a representation of logs; the data structure of traces looks almost like that of an event log. A single trace can provide visibility into both the path traversed by a request as well as the structure of a request. The path of a request allows software engineers and SREs to understand the different services involved in the path of a request, and the structure of a request helps one understand the junctures and effects of asynchrony in the execution of a request.

=== The Challenges of Tracing
Tracing is, by far, the hardest to retrofit into an existing infrastructure, because for tracing to be truly effective, every component in the path of a request needs to be modified to propagate tracing information. Depending on whom you ask, you’d either be told that having gaps in the flow of a request doesn’t outweigh the cons (since adding tracing piecemeal is seen as better than having no tracing at all, as having partial tracing helps eke out nuggets of knowledge from the fog of war) or be told that these gaps are blind spots that make debugging harder.

The second problem with tracing instrumentation is that it’s not sufficient for developers to instrument their code alone. A large number of applications in the wild are built using open source frameworks or libraries that might require additional instrumentation. This becomes all the more challenging at places with polyglot architectures, since every language, framework, and wire protocol with widely disparate concurrency patterns and guarantees needs to cooperate. Indeed, tracing is most successfully deployed in organizations that use a core set of languages and frameworks uniformly across the company.

The cost of tracing isn’t quite as catastrophic as that of logging, mainly because traces are almost always sampled heavily to reduce runtime overhead as well as storage costs. Sampling decisions can be made:

- At the start of a request before any traces are generated

- At the end, after all participating systems have recorded the traces for the entire course of the request execution

- Midway through the request flow, when only downstream services would then report the trace

All approaches have their own pros and cons, and one might even want to use them all.
<<<

== Further readings

=== Conferences/Videos

- https://www.infoq.com/presentations/cncf-open-telemetry-fluentbit/?itm_campaign=rightbar_v2&itm_source=infoq&itm_medium=presentations_link&itm_content=link_text[InfoQ - Embracing Observability in Distributed Systems]

== About OpenTelemetry

Initially the OpenTelemetry community took on Distributed Tracing. Metrics and Logs are still in progress. A complete observability story includes all three pillars, but currently our Azure Monitor OpenTelemetry-based exporter preview offerings for .NET, Python, and JavaScript only include Distributed Tracing.




